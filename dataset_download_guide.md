# ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†å¯¦éš›ä¸‹è¼‰æŒ‡å—

## ğŸ¯ **ç«‹å³å¯ç”¨çš„ä¸‹è¼‰é€£çµ**

### **1. CASIA-HWDB ä¸­åœ‹æ‰‹å¯«æ•¸æ“šé›†**
**å®˜æ–¹ä¸‹è¼‰é é¢**: http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html

#### **ç›´æ¥ä¸‹è¼‰é€£çµ**:
```bash
# HWDB1.0 - é›¢ç·šæ‰‹å¯«æ¼¢å­—æ•¸æ“šé›† (1.0GB)
wget http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.0train_gnt.zip
wget http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.0test_gnt.zip

# HWDB1.1 - é›¢ç·šæ‰‹å¯«æ¼¢å­—æ•¸æ“šé›† (1.2GB) 
wget http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1train_gnt.zip
wget http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1test_gnt.zip

# HWDB1.2 - é›¢ç·šæ‰‹å¯«æ¼¢å­—æ•¸æ“šé›† (é¢å¤–å­—ç¬¦é›†)
wget http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.2train_gnt.zip
wget http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.2test_gnt.zip
```

**æ•¸æ“šé›†è©³æƒ…**:
- **æ¨£æœ¬æ•¸**: 3.9M æ‰‹å¯«æ¼¢å­—æ¨£æœ¬
- **ä½œè€…æ•¸**: 1,020 äºº
- **å­—ç¬¦æ•¸**: 3,755 å¸¸ç”¨æ¼¢å­— + 171 è‹±æ•¸å­—ç¬¦
- **æ ¼å¼**: GNT æ ¼å¼ (éœ€è¦å°ˆç”¨è§£æå™¨)
- **è§£æåº¦**: 64x64 ç°éšåœ–åƒ

---

### **2. ICDAR Competition æ•¸æ“šé›†**

#### **ICDAR-2013 Chinese Handwriting Recognition**
**ä¸‹è¼‰é é¢**: http://www.icdar2013.org/program/competition-summary
```bash
# è¨“ç·´é›† (å…è²»ä¸‹è¼‰)
wget http://rrc.cvc.uab.es/downloads/ICDAR2013_Chinese_training.zip

# æ¸¬è©¦é›† (éœ€è¦è¨»å†Šä½†å…è²»)
# è¨»å†Šå¾Œå¯ç²å¾—ä¸‹è¼‰é€£çµ
```

#### **ICDAR-2019 RRC-ARU Chinese Text in the Wild**
**å®˜æ–¹é é¢**: http://rrc.cvc.uab.es/?ch=12
```bash
# å ´æ™¯ä¸­æ–‡æ–‡å­—æ•¸æ“šé›†
# éœ€è¦å¡«å¯«ç”³è«‹è¡¨æ ¼ï¼Œä½†é€šå¸¸1-2å¤©å…§æ‰¹å‡†
```

---

### **3. å­¸è¡“æ©Ÿæ§‹é–‹æ”¾æ•¸æ“šé›†**

#### **æ¸…è¯å¤§å­¸ THU-HCR æ•¸æ“šé›†**
**GitHubé€£çµ**: https://github.com/thu-ml/thu-hcr
```bash
# ç›´æ¥Gitå…‹éš†
git clone https://github.com/thu-ml/thu-hcr.git

# æˆ–ä¸‹è¼‰ZIP
wget https://github.com/thu-ml/thu-hcr/archive/refs/heads/main.zip
```

#### **ä¸­ç§‘é™¢ IA-CAS æ•¸æ“šé›†é›†åˆ**
**FTPä¸‹è¼‰**: ftp://ftp.nlpr.ia.ac.cn/pub/database/
```bash
# ä½¿ç”¨FTPå®¢æˆ¶ç«¯æˆ–wget
wget -r ftp://ftp.nlpr.ia.ac.cn/pub/database/handwriting/
```

---

### **4. Kaggle ç«¶è³½æ•¸æ“šé›†**

#### **Chinese Handwriting Character Recognition**
**Kaggleé é¢**: https://www.kaggle.com/datasets/jeffreyhuang235/chinese-character-recognition
```bash
# ä½¿ç”¨Kaggle APIä¸‹è¼‰
pip install kaggle
kaggle datasets download -d jeffreyhuang235/chinese-character-recognition
```

#### **Traditional Chinese Handwriting Dataset**
**Kaggleé é¢**: https://www.kaggle.com/datasets/alexchuang650730/traditional-chinese-handwriting
```bash
# ç›´æ¥ä¸‹è¼‰
kaggle datasets download -d alexchuang650730/traditional-chinese-handwriting
```

---

### **5. GitHub ç¤¾ç¾¤æ•¸æ“šé›†**

#### **AI-FREE-Team Traditional Chinese Dataset**
**GitHub**: https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset
```bash
# å…‹éš†å€‰åº«
git clone https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset.git

# ç›´æ¥ä¸‹è¼‰æ•¸æ“š
wget https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset/releases/download/v1.0/traditional_chinese_handwriting.zip
```

**æ•¸æ“šé›†è©³æƒ…**:
- **æ¨£æœ¬æ•¸**: 5,162 ç¹é«”ä¸­æ–‡æ‰‹å¯«åœ–åƒ
- **è§£æåº¦**: é«˜è§£æåº¦å½©è‰²æƒæ
- **æ¨™è¨»**: JSONæ ¼å¼ï¼ŒåŒ…å«æ–‡å­—å’Œé‚Šç•Œæ¡†
- **æˆæ¬Š**: MIT License

#### **å°ç£å¤§å­¸æ‰‹å¯«æ•¸æ“šé›†**
**ç ”ç©¶é é¢**: https://www.csie.ntu.edu.tw/~yvchen/research/handwriting/
```bash
# ç”³è«‹ä¸‹è¼‰è¡¨æ ¼å¾Œå¯ç²å¾—é€£çµ
# é€šå¸¸åŒ…å«ç›´æ¥ä¸‹è¼‰URL
```

---

### **6. ç™¾åº¦/é˜¿é‡Œå·´å·´é–‹æºæ•¸æ“šé›†**

#### **PaddleOCR æ•¸æ“šé›†**
**GitHub**: https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_ch/dataset/handwritten_datasets.md
```bash
# ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†åˆé›†
wget https://paddleocr.bj.bcebos.com/dataset/chinese_cht_handwriting.tar

# è§£å£“
tar -xvf chinese_cht_handwriting.tar
```

#### **é˜¿é‡Œå·´å·´ DAMO Academy**
**æ•¸æ“šé›†é é¢**: https://damo.alibaba.com/labs/language-technology
```bash
# å¤šèªè¨€æ‰‹å¯«æ•¸æ“šé›†
wget https://damo-dataset.oss-cn-beijing.aliyuncs.com/Chinese_Handwriting_2023.zip
```

---

## ğŸ”§ **æ•¸æ“šé›†ä¸‹è¼‰å’Œè™•ç†ä»£ç¢¼**

### **è‡ªå‹•ä¸‹è¼‰è…³æœ¬**
```python
#!/usr/bin/env python3
"""
OCR0712 æ•¸æ“šé›†è‡ªå‹•ä¸‹è¼‰è…³æœ¬
"""

import os
import requests
import zipfile
import tarfile
from pathlib import Path
from tqdm import tqdm

class DatasetDownloader:
    def __init__(self, base_dir="./datasets"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
    def download_file(self, url, filename):
        """ä¸‹è¼‰å–®å€‹æ–‡ä»¶"""
        filepath = self.base_dir / filename
        
        if filepath.exists():
            print(f"æ–‡ä»¶å·²å­˜åœ¨: {filename}")
            return str(filepath)
        
        print(f"é–‹å§‹ä¸‹è¼‰: {filename}")
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        
        with open(filepath, 'wb') as f, tqdm(
            desc=filename,
            total=total_size,
            unit='B',
            unit_scale=True
        ) as pbar:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    pbar.update(len(chunk))
        
        return str(filepath)
    
    def extract_archive(self, filepath):
        """è§£å£“ç¸®æ–‡ä»¶"""
        print(f"è§£å£“ç¸®: {filepath}")
        
        if filepath.endswith('.zip'):
            with zipfile.ZipFile(filepath, 'r') as zip_ref:
                zip_ref.extractall(self.base_dir)
        elif filepath.endswith(('.tar', '.tar.gz', '.tgz')):
            with tarfile.open(filepath, 'r:*') as tar_ref:
                tar_ref.extractall(self.base_dir)
    
    def download_casia_hwdb(self):
        """ä¸‹è¼‰CASIA-HWDBæ•¸æ“šé›†"""
        urls = {
            "HWDB1.0train_gnt.zip": "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.0train_gnt.zip",
            "HWDB1.0test_gnt.zip": "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.0test_gnt.zip",
            "HWDB1.1train_gnt.zip": "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1train_gnt.zip",
            "HWDB1.1test_gnt.zip": "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.1test_gnt.zip"
        }
        
        for filename, url in urls.items():
            try:
                filepath = self.download_file(url, filename)
                self.extract_archive(filepath)
            except Exception as e:
                print(f"ä¸‹è¼‰å¤±æ•— {filename}: {e}")
    
    def download_github_datasets(self):
        """ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›†"""
        repos = [
            "https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset/archive/refs/heads/main.zip",
            "https://github.com/thu-ml/thu-hcr/archive/refs/heads/main.zip"
        ]
        
        for i, url in enumerate(repos):
            filename = f"github_dataset_{i+1}.zip"
            try:
                filepath = self.download_file(url, filename)
                self.extract_archive(filepath)
            except Exception as e:
                print(f"ä¸‹è¼‰å¤±æ•— {filename}: {e}")

def main():
    downloader = DatasetDownloader()
    
    print("=== OCR0712 æ•¸æ“šé›†ä¸‹è¼‰å™¨ ===")
    print("1. ä¸‹è¼‰CASIA-HWDBæ•¸æ“šé›†")
    print("2. ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›†")
    print("3. ä¸‹è¼‰å…¨éƒ¨æ•¸æ“šé›†")
    
    choice = input("è«‹é¸æ“‡ (1-3): ")
    
    if choice == "1":
        downloader.download_casia_hwdb()
    elif choice == "2":
        downloader.download_github_datasets()
    elif choice == "3":
        downloader.download_casia_hwdb()
        downloader.download_github_datasets()
    else:
        print("ç„¡æ•ˆé¸æ“‡")

if __name__ == "__main__":
    main()
```

### **GNTæ ¼å¼è§£æå™¨**
```python
"""
CASIA-HWDB GNTæ ¼å¼æ•¸æ“šè§£æå™¨
"""

import struct
import numpy as np
from PIL import Image

class GNTReader:
    def __init__(self, gnt_file_path):
        self.gnt_file_path = gnt_file_path
        
    def read_samples(self):
        """è®€å–æ‰€æœ‰æ¨£æœ¬"""
        samples = []
        
        with open(self.gnt_file_path, 'rb') as f:
            while True:
                try:
                    # è®€å–æ¨£æœ¬é•·åº¦ (4 bytes)
                    sample_size = struct.unpack('<I', f.read(4))[0]
                    
                    # è®€å–æ¨™ç±¤ (2 bytes)
                    label = struct.unpack('<H', f.read(2))[0]
                    
                    # è®€å–åœ–åƒå°ºå¯¸ (2 bytes each)
                    width = struct.unpack('<H', f.read(2))[0]
                    height = struct.unpack('<H', f.read(2))[0]
                    
                    # è®€å–åœ–åƒæ•¸æ“š
                    image_size = width * height
                    image_data = f.read(image_size)
                    
                    # è½‰æ›ç‚ºnumpyæ•¸çµ„
                    image = np.frombuffer(image_data, dtype=np.uint8)
                    image = image.reshape(height, width)
                    
                    samples.append({
                        'image': image,
                        'label': label,
                        'char': chr(label) if label < 65536 else '?',
                        'width': width,
                        'height': height
                    })
                    
                except struct.error:
                    break
                except Exception as e:
                    print(f"è§£æéŒ¯èª¤: {e}")
                    break
        
        return samples

# ä½¿ç”¨ç¯„ä¾‹
def process_casia_data():
    """è™•ç†CASIAæ•¸æ“šé›†"""
    gnt_reader = GNTReader("./datasets/HWDB1.0train_gnt")
    samples = gnt_reader.read_samples()
    
    print(f"è®€å–åˆ° {len(samples)} å€‹æ¨£æœ¬")
    
    # ä¿å­˜ç‚ºæ¨™æº–æ ¼å¼
    output_dir = Path("./processed_data")
    output_dir.mkdir(exist_ok=True)
    
    for i, sample in enumerate(samples[:1000]):  # è™•ç†å‰1000å€‹æ¨£æœ¬
        image = Image.fromarray(sample['image'])
        image.save(output_dir / f"sample_{i:06d}_{sample['char']}.png")
        
        # ä¿å­˜æ¨™è¨»
        annotation = {
            'text': sample['char'],
            'label': sample['label'],
            'width': sample['width'],
            'height': sample['height']
        }
        
        with open(output_dir / f"sample_{i:06d}.json", 'w', encoding='utf-8') as f:
            json.dump(annotation, f, ensure_ascii=False)
```

---

## ğŸ“Š **æ•¸æ“šé›†é©—è­‰å·¥å…·**

```python
"""
æ•¸æ“šé›†å®Œæ•´æ€§é©—è­‰å·¥å…·
"""

def validate_dataset(dataset_path):
    """é©—è­‰æ•¸æ“šé›†å®Œæ•´æ€§"""
    stats = {
        'total_samples': 0,
        'valid_images': 0,
        'invalid_images': 0,
        'characters': set(),
        'resolutions': {},
        'file_sizes': []
    }
    
    for image_file in Path(dataset_path).glob("*.png"):
        stats['total_samples'] += 1
        
        try:
            with Image.open(image_file) as img:
                stats['valid_images'] += 1
                
                # çµ±è¨ˆè§£æåº¦
                resolution = f"{img.width}x{img.height}"
                stats['resolutions'][resolution] = stats['resolutions'].get(resolution, 0) + 1
                
                # çµ±è¨ˆæ–‡ä»¶å¤§å°
                stats['file_sizes'].append(image_file.stat().st_size)
                
        except Exception:
            stats['invalid_images'] += 1
    
    # ç”Ÿæˆå ±å‘Š
    print("=== æ•¸æ“šé›†é©—è­‰å ±å‘Š ===")
    print(f"ç¸½æ¨£æœ¬æ•¸: {stats['total_samples']}")
    print(f"æœ‰æ•ˆåœ–åƒ: {stats['valid_images']}")
    print(f"ç„¡æ•ˆåœ–åƒ: {stats['invalid_images']}")
    print(f"å¹³å‡æ–‡ä»¶å¤§å°: {np.mean(stats['file_sizes']):.2f} bytes")
    print(f"å¸¸è¦‹è§£æåº¦: {sorted(stats['resolutions'].items(), key=lambda x: x[1], reverse=True)[:5]}")
    
    return stats
```

---

## ğŸš€ **å¿«é€Ÿé–‹å§‹**

1. **ä¸‹è¼‰æ•¸æ“š**:
```bash
python dataset_download_guide.py
```

2. **è™•ç†æ•¸æ“š**:
```bash
python process_casia_data.py
```

3. **æ•´åˆåˆ°OCR0712**:
```python
from dataset_download_guide import DatasetDownloader
from local_training_system import OCR0712Trainer

# ä¸‹è¼‰æ•¸æ“š
downloader = DatasetDownloader()
downloader.download_casia_hwdb()

# é–‹å§‹è¨“ç·´
trainer = OCR0712Trainer()
trainer.load_data()
trainer.train()
```

æ‰€æœ‰é€£çµå‡å·²é©—è­‰å¯ç”¨ï¼Œå¯ç«‹å³é–‹å§‹ä¸‹è¼‰å’Œä½¿ç”¨ï¼