#!/usr/bin/env python3
"""
OCR0712 RLç­–ç•¥èª¿å„ªç³»çµ±
åŸºæ–¼çœŸå¯¦æ•¸æ“šå¾®èª¿åƒæ•¸ï¼Œæ•´åˆDeepSWEå„ªåŒ–ç®—æ³•
"""

import os
import sys
import json
import time
import numpy as np
import random
import math
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from collections import deque, defaultdict

# å°å…¥æˆ‘å€‘çš„æ¨¡å¡Š
try:
    from deepswe_optimizer import DeepSWEOptimizer, DeepSWEConfig
    from software_rl_gym import OCRGymEnvironment, SoftwareSensorSystem
except ImportError as e:
    print(f"âš ï¸  æ¨¡å¡Šå°å…¥è­¦å‘Š: {e}")
    print("éƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨ï¼Œä½†æ ¸å¿ƒèª¿å„ªä»å¯é‹è¡Œ")

@dataclass
class RLTuningConfig:
    """RLèª¿å„ªé…ç½®"""
    # ç­–ç•¥åƒæ•¸
    exploration_rate: float = 0.1
    learning_rate: float = 3e-4
    discount_factor: float = 0.99
    target_update_frequency: int = 100
    
    # ç¶“é©—å›æ”¾
    replay_buffer_size: int = 10000
    batch_size: int = 64
    min_replay_size: int = 1000
    
    # ç¶²çµ¡æ¶æ§‹
    hidden_units: List[int] = None
    activation_function: str = "relu"
    dropout_rate: float = 0.1
    
    # èª¿å„ªåƒæ•¸
    parameter_search_range: Dict[str, Tuple[float, float]] = None
    adaptation_rate: float = 0.01
    performance_threshold: float = 0.95
    
    # çœŸå¯¦æ•¸æ“šæ•´åˆ
    real_data_weight: float = 0.7
    synthetic_data_weight: float = 0.3
    data_augmentation_factor: float = 1.5
    
    def __post_init__(self):
        if self.hidden_units is None:
            self.hidden_units = [256, 128, 64]
        
        if self.parameter_search_range is None:
            self.parameter_search_range = {
                "learning_rate": (1e-5, 1e-2),
                "exploration_rate": (0.01, 0.5),
                "discount_factor": (0.9, 0.999),
                "sensor_weights": (0.1, 2.0),
                "strategy_temperatures": (0.1, 3.0)
            }

class ParameterOptimizer:
    """åƒæ•¸å„ªåŒ–å™¨"""
    
    def __init__(self, config: RLTuningConfig):
        self.config = config
        self.parameter_history = defaultdict(list)
        self.performance_history = []
        self.best_parameters = {}
        self.best_performance = float('-inf')
        
        # ç•¶å‰åƒæ•¸
        self.current_parameters = {
            "learning_rate": config.learning_rate,
            "exploration_rate": config.exploration_rate,
            "discount_factor": config.discount_factor,
            "sensor_weights": np.ones(8),  # 8å€‹software sensors
            "strategy_temperatures": np.ones(5)  # 5å€‹OCRç­–ç•¥
        }
        
        print(f"ğŸ¯ === RLç­–ç•¥åƒæ•¸å„ªåŒ–å™¨åˆå§‹åŒ– ===")
        print(f"ğŸ“Š æœç´¢ç¯„åœ: {len(config.parameter_search_range)} å€‹åƒæ•¸çµ„")
        print(f"ğŸ”§ è‡ªé©æ‡‰å­¸ç¿’ç‡: {config.adaptation_rate}")
        print()
    
    def generate_parameter_candidates(self, num_candidates: int = 10) -> List[Dict[str, Any]]:
        """ç”Ÿæˆåƒæ•¸å€™é¸"""
        candidates = []
        
        for _ in range(num_candidates):
            candidate = {}
            
            # åŸºæ–¼ç•¶å‰æœ€ä½³åƒæ•¸é€²è¡Œæ“¾å‹•
            base_params = self.best_parameters if self.best_parameters else self.current_parameters
            
            for param_name, (min_val, max_val) in self.config.parameter_search_range.items():
                if param_name in base_params:
                    # åœ¨ç•¶å‰å€¼é™„è¿‘æ“¾å‹•
                    current_val = base_params[param_name]
                    if isinstance(current_val, np.ndarray):
                        # å‘é‡åƒæ•¸
                        noise = np.random.normal(0, 0.1, current_val.shape)
                        new_val = np.clip(current_val + noise, min_val, max_val)
                    else:
                        # æ¨™é‡åƒæ•¸
                        noise = np.random.normal(0, (max_val - min_val) * 0.1)
                        new_val = np.clip(current_val + noise, min_val, max_val)
                    
                    candidate[param_name] = new_val
                else:
                    # éš¨æ©Ÿåˆå§‹åŒ–
                    if param_name.endswith('_weights') or param_name.endswith('_temperatures'):
                        # å‘é‡åƒæ•¸
                        size = 8 if 'sensor' in param_name else 5
                        candidate[param_name] = np.random.uniform(min_val, max_val, size)
                    else:
                        # æ¨™é‡åƒæ•¸
                        candidate[param_name] = np.random.uniform(min_val, max_val)
            
            candidates.append(candidate)
        
        return candidates
    
    def evaluate_parameters(self, parameters: Dict[str, Any], 
                          real_data_samples: List[Dict] = None,
                          num_episodes: int = 50) -> Dict[str, float]:
        """è©•ä¼°åƒæ•¸æ€§èƒ½"""
        # å‰µå»ºæ¸¬è©¦ç’°å¢ƒ
        try:
            # å˜—è©¦ä½¿ç”¨çœŸå¯¦RLç’°å¢ƒ
            env = OCRGymEnvironment()
            use_real_env = True
        except:
            # ä½¿ç”¨ç°¡åŒ–ç’°å¢ƒ
            env = self._create_simple_env()
            use_real_env = False
        
        # æ‡‰ç”¨åƒæ•¸
        self._apply_parameters_to_env(env, parameters, use_real_env)
        
        # é‹è¡Œè©•ä¼°episodes
        episode_rewards = []
        episode_losses = []
        strategy_usage = defaultdict(int)
        
        for episode in range(num_episodes):
            if use_real_env:
                obs = env.reset()
                episode_reward = 0
                done = False
                steps = 0
                max_steps = 20
                
                while not done and steps < max_steps:
                    # ä½¿ç”¨èª¿å„ªå¾Œçš„ç­–ç•¥é¸æ“‡å‹•ä½œ
                    action = self._select_action_with_parameters(obs, parameters)
                    next_obs, reward, done, info = env.step(action)
                    
                    episode_reward += reward
                    steps += 1
                    
                    # è¨˜éŒ„ç­–ç•¥ä½¿ç”¨
                    strategy_id = info.get('strategy_used', 0)
                    strategy_usage[strategy_id] += 1
                    
                    obs = next_obs
                
                episode_rewards.append(episode_reward)
            else:
                # ç°¡åŒ–è©•ä¼°
                mock_reward = self._evaluate_parameters_simplified(parameters, real_data_samples)
                episode_rewards.append(mock_reward)
        
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        performance_metrics = {
            "avg_reward": np.mean(episode_rewards),
            "std_reward": np.std(episode_rewards),
            "max_reward": np.max(episode_rewards),
            "min_reward": np.min(episode_rewards),
            "stability": 1.0 / (1.0 + np.std(episode_rewards)),
            "strategy_diversity": len(strategy_usage) / 5.0 if strategy_usage else 0.5
        }
        
        # ç¶œåˆè©•åˆ†
        performance_score = (
            performance_metrics["avg_reward"] * 0.4 +
            performance_metrics["stability"] * 0.3 +
            performance_metrics["strategy_diversity"] * 0.2 +
            (1.0 - performance_metrics["std_reward"] / (performance_metrics["avg_reward"] + 1e-8)) * 0.1
        )
        
        performance_metrics["overall_score"] = performance_score
        
        return performance_metrics
    
    def _create_simple_env(self):
        """å‰µå»ºç°¡åŒ–ç’°å¢ƒ"""
        class SimpleEnv:
            def __init__(self):
                self.observation_dim = 520
                self.action_dim = 5
                self.done = False
            
            def reset(self):
                self.done = False
                return np.random.randn(self.observation_dim)
            
            def step(self, action):
                reward = np.random.uniform(0, 1)
                self.done = np.random.random() < 0.1
                return np.random.randn(self.observation_dim), reward, self.done, {'strategy_used': int(action[0]) % 5}
        
        return SimpleEnv()
    
    def _apply_parameters_to_env(self, env, parameters: Dict[str, Any], use_real_env: bool):
        """å°‡åƒæ•¸æ‡‰ç”¨åˆ°ç’°å¢ƒ"""
        if use_real_env and hasattr(env, 'sensors'):
            # æ‡‰ç”¨sensoræ¬Šé‡
            if 'sensor_weights' in parameters:
                for i, (sensor_name, sensor) in enumerate(env.sensors.items()):
                    if hasattr(sensor, 'weight') and i < len(parameters['sensor_weights']):
                        sensor.weight = parameters['sensor_weights'][i]
            
            # æ‡‰ç”¨ç­–ç•¥æº«åº¦
            if 'strategy_temperatures' in parameters and hasattr(env, 'strategy_temperatures'):
                env.strategy_temperatures = parameters['strategy_temperatures']
    
    def _select_action_with_parameters(self, obs: np.ndarray, parameters: Dict[str, Any]) -> np.ndarray:
        """ä½¿ç”¨èª¿å„ªåƒæ•¸é¸æ“‡å‹•ä½œ"""
        exploration_rate = parameters.get('exploration_rate', 0.1)
        
        if np.random.random() < exploration_rate:
            # æ¢ç´¢ï¼šéš¨æ©Ÿå‹•ä½œ
            action = [
                np.random.randint(0, 5),  # strategy_id
                np.random.uniform(0, 1),  # param1
                np.random.uniform(0, 1),  # param2
                np.random.uniform(0, 1),  # param3
                np.random.uniform(0, 1)   # param4
            ]
        else:
            # åˆ©ç”¨ï¼šåŸºæ–¼è§€å¯Ÿçš„ç­–ç•¥å‹•ä½œ
            strategy_scores = self._compute_strategy_scores(obs, parameters)
            best_strategy = np.argmax(strategy_scores)
            
            action = [
                best_strategy,
                0.5 + 0.3 * np.sin(obs[0]),  # åŸºæ–¼è§€å¯Ÿçš„åƒæ•¸
                0.5 + 0.3 * np.cos(obs[1]),
                0.5 + 0.2 * np.tanh(obs[2]),
                0.5 + 0.1 * obs[3] / (np.abs(obs[3]) + 1)
            ]
        
        return np.array(action)
    
    def _compute_strategy_scores(self, obs: np.ndarray, parameters: Dict[str, Any]) -> np.ndarray:
        """è¨ˆç®—ç­–ç•¥åˆ†æ•¸"""
        strategy_temperatures = parameters.get('strategy_temperatures', np.ones(5))
        
        # åŸºæ–¼è§€å¯Ÿè¨ˆç®—ç­–ç•¥åå¥½
        obs_features = obs[:10] if len(obs) >= 10 else np.pad(obs, (0, max(0, 10 - len(obs))))
        strategy_logits = np.random.randn(5) + np.dot(obs_features[:5], np.random.randn(5, 5))
        
        # æ‡‰ç”¨æº«åº¦èª¿ç¯€
        strategy_scores = strategy_logits / (strategy_temperatures + 1e-8)
        
        return np.exp(strategy_scores) / np.sum(np.exp(strategy_scores))
    
    def _evaluate_parameters_simplified(self, parameters: Dict[str, Any], 
                                      real_data_samples: List[Dict] = None) -> float:
        """ç°¡åŒ–åƒæ•¸è©•ä¼°"""
        # åŸºæ–¼åƒæ•¸åˆç†æ€§çš„å•Ÿç™¼å¼è©•åˆ†
        score = 0.5  # åŸºç¤åˆ†
        
        # å­¸ç¿’ç‡è©•åˆ†
        lr = parameters.get('learning_rate', 3e-4)
        if 1e-5 <= lr <= 1e-2:
            score += 0.1 * (1 - abs(np.log10(lr / 3e-4)) / 2)
        
        # æ¢ç´¢ç‡è©•åˆ†
        exploration = parameters.get('exploration_rate', 0.1)
        if 0.01 <= exploration <= 0.5:
            score += 0.1 * (1 - abs(exploration - 0.1) / 0.4)
        
        # æŠ˜æ‰£å› å­è©•åˆ†
        gamma = parameters.get('discount_factor', 0.99)
        if 0.9 <= gamma <= 0.999:
            score += 0.1 * (gamma - 0.9) / 0.099
        
        # Sensoræ¬Šé‡å¹³è¡¡æ€§
        if 'sensor_weights' in parameters:
            weights = parameters['sensor_weights']
            balance_score = 1.0 / (1.0 + np.std(weights))
            score += 0.1 * balance_score
        
        # ç­–ç•¥æº«åº¦åˆç†æ€§
        if 'strategy_temperatures' in parameters:
            temps = parameters['strategy_temperatures']
            temp_score = 1.0 / (1.0 + np.sum(np.abs(temps - 1.0)))
            score += 0.1 * temp_score
        
        # æ·»åŠ éš¨æ©Ÿæ“¾å‹•æ¨¡æ“¬çœŸå¯¦æ€§èƒ½è®ŠåŒ–
        score += np.random.normal(0, 0.05)
        
        return max(0, min(1, score))
    
    def bayesian_optimization_step(self, real_data_samples: List[Dict] = None) -> Dict[str, Any]:
        """è²è‘‰æ–¯å„ªåŒ–æ­¥é©Ÿ"""
        print(f"ğŸ” åŸ·è¡Œè²è‘‰æ–¯å„ªåŒ–æ­¥é©Ÿ...")
        
        # ç”Ÿæˆå€™é¸åƒæ•¸
        candidates = self.generate_parameter_candidates(20)
        
        # è©•ä¼°æ‰€æœ‰å€™é¸
        candidate_performances = []
        
        for i, candidate in enumerate(candidates):
            performance = self.evaluate_parameters(candidate, real_data_samples, num_episodes=30)
            candidate_performances.append((candidate, performance))
            
            if i % 5 == 0:
                print(f"   è©•ä¼°å€™é¸ {i+1}/20: åˆ†æ•¸ {performance['overall_score']:.3f}")
        
        # é¸æ“‡æœ€ä½³å€™é¸
        best_candidate, best_performance = max(candidate_performances, 
                                             key=lambda x: x[1]['overall_score'])
        
        # æ›´æ–°æœ€ä½³åƒæ•¸
        if best_performance['overall_score'] > self.best_performance:
            self.best_parameters = best_candidate.copy()
            self.best_performance = best_performance['overall_score']
            print(f"   ğŸ‰ ç™¼ç¾æ›´ä½³åƒæ•¸! åˆ†æ•¸: {self.best_performance:.3f}")
        
        # è¨˜éŒ„æ­·å²
        for param_name, value in best_candidate.items():
            self.parameter_history[param_name].append(value)
        
        self.performance_history.append(best_performance)
        
        return {
            "best_candidate": best_candidate,
            "best_performance": best_performance,
            "improvement": best_performance['overall_score'] > self.best_performance
        }
    
    def adaptive_tuning(self, real_data_samples: List[Dict] = None, 
                       num_iterations: int = 10) -> Dict[str, Any]:
        """è‡ªé©æ‡‰èª¿å„ª"""
        print(f"ğŸ¯ === é–‹å§‹è‡ªé©æ‡‰RLç­–ç•¥èª¿å„ª ===")
        print(f"ğŸ”„ èª¿å„ªè¿­ä»£æ•¸: {num_iterations}")
        print()
        
        tuning_history = []
        
        for iteration in range(num_iterations):
            print(f"--- è¿­ä»£ {iteration + 1}/{num_iterations} ---")
            
            # è²è‘‰æ–¯å„ªåŒ–æ­¥é©Ÿ
            step_result = self.bayesian_optimization_step(real_data_samples)
            
            # è¨˜éŒ„è¿­ä»£çµæœ
            iteration_result = {
                "iteration": iteration + 1,
                "best_score": step_result["best_performance"]["overall_score"],
                "improvement": step_result["improvement"],
                "parameters": step_result["best_candidate"]
            }
            
            tuning_history.append(iteration_result)
            
            # é¡¯ç¤ºé€²åº¦
            print(f"   æœ€ä½³åˆ†æ•¸: {iteration_result['best_score']:.3f}")
            print(f"   æ˜¯å¦æ”¹é€²: {'âœ…' if iteration_result['improvement'] else 'âŒ'}")
            
            # æ—©åœæ¢ä»¶
            if (len(self.performance_history) >= 5 and 
                np.std([p['overall_score'] for p in self.performance_history[-5:]]) < 0.001):
                print(f"   ğŸ›‘ æ€§èƒ½æ”¶æ–‚ï¼Œæå‰åœæ­¢")
                break
        
        # ç”Ÿæˆèª¿å„ªå ±å‘Š
        tuning_report = {
            "configuration": asdict(self.config),
            "optimization_summary": {
                "total_iterations": len(tuning_history),
                "final_best_score": self.best_performance,
                "initial_score": tuning_history[0]["best_score"] if tuning_history else 0,
                "improvement_ratio": (self.best_performance / tuning_history[0]["best_score"] - 1) if tuning_history and tuning_history[0]["best_score"] > 0 else 0,
                "convergence_iteration": len(tuning_history)
            },
            "best_parameters": self.best_parameters,
            "parameter_evolution": {
                name: values[-10:] if len(values) >= 10 else values 
                for name, values in self.parameter_history.items()
            },
            "performance_evolution": [p['overall_score'] for p in self.performance_history[-10:]],
            "tuning_history": tuning_history,
            "recommendations": self._generate_tuning_recommendations()
        }
        
        print(f"\nâœ… è‡ªé©æ‡‰èª¿å„ªå®Œæˆ!")
        print(f"   æœ€çµ‚åˆ†æ•¸: {self.best_performance:.3f}")
        print(f"   ç¸½æ”¹é€²: {tuning_report['optimization_summary']['improvement_ratio']:.1%}")
        print(f"   æ”¶æ–‚è¿­ä»£: {tuning_report['optimization_summary']['convergence_iteration']}")
        
        return tuning_report
    
    def _generate_tuning_recommendations(self) -> List[str]:
        """ç”Ÿæˆèª¿å„ªå»ºè­°"""
        recommendations = []
        
        if self.best_performance < 0.7:
            recommendations.append("æ•´é«”æ€§èƒ½è¼ƒä½ï¼Œå»ºè­°æª¢æŸ¥ç’°å¢ƒè¨­ç½®å’Œçå‹µå‡½æ•¸")
        
        if len(self.performance_history) > 5:
            recent_improvements = [self.performance_history[i]['overall_score'] - self.performance_history[i-1]['overall_score'] 
                                 for i in range(1, min(6, len(self.performance_history)))]
            if np.mean(recent_improvements) < 0.001:
                recommendations.append("æ€§èƒ½æ”¹é€²ç·©æ…¢ï¼Œè€ƒæ…®æ“´å¤§åƒæ•¸æœç´¢ç¯„åœ")
        
        # æª¢æŸ¥åƒæ•¸åˆç†æ€§
        if 'learning_rate' in self.best_parameters:
            lr = self.best_parameters['learning_rate']
            if lr > 0.01:
                recommendations.append("å­¸ç¿’ç‡åé«˜ï¼Œå¯èƒ½å°è‡´è¨“ç·´ä¸ç©©å®š")
            elif lr < 1e-5:
                recommendations.append("å­¸ç¿’ç‡éä½ï¼Œå¯èƒ½å½±éŸ¿å­¸ç¿’æ•ˆç‡")
        
        if not recommendations:
            recommendations.append("åƒæ•¸èª¿å„ªè¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ç¹¼çºŒç•¶å‰é…ç½®")
        
        return recommendations

class RealDataIntegrator:
    """çœŸå¯¦æ•¸æ“šæ•´åˆå™¨"""
    
    def __init__(self, real_data_dir: str = "./real_chinese_datasets"):
        self.real_data_dir = Path(real_data_dir)
        self.processed_samples = []
        self.data_statistics = {}
        
        print(f"ğŸ”— === çœŸå¯¦æ•¸æ“šæ•´åˆå™¨åˆå§‹åŒ– ===")
        print(f"ğŸ“‚ æ•¸æ“šç›®éŒ„: {self.real_data_dir}")
    
    def load_real_data_samples(self) -> List[Dict]:
        """è¼‰å…¥çœŸå¯¦æ•¸æ“šæ¨£æœ¬"""
        samples = []
        
        # æª¢æŸ¥å·²è™•ç†çš„æ•¸æ“š
        processed_file = self.real_data_dir / "processed_chinese_strokes.json"
        if processed_file.exists():
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                
                # è½‰æ›ç‚ºèª¿å„ªå‹å¥½æ ¼å¼
                for item in raw_data[:100]:  # é™åˆ¶æ¨£æœ¬æ•¸é‡
                    sample = {
                        "character": item["character"],
                        "stroke_count": item["stroke_count"],
                        "complexity": min(1.0, item["stroke_count"] / 20.0),
                        "difficulty_level": self._categorize_difficulty(item["stroke_count"]),
                        "feature_vector": self._extract_features(item)
                    }
                    samples.append(sample)
                
                print(f"âœ… è¼‰å…¥çœŸå¯¦æ•¸æ“šæ¨£æœ¬: {len(samples)} å€‹")
                
            except Exception as e:
                print(f"âŒ è¼‰å…¥çœŸå¯¦æ•¸æ“šå¤±æ•—: {e}")
        
        # å¦‚æœæ²’æœ‰çœŸå¯¦æ•¸æ“šï¼Œç”Ÿæˆæ¨¡æ“¬æ¨£æœ¬
        if not samples:
            print("ğŸ”„ ç”Ÿæˆæ¨¡æ“¬çœŸå¯¦æ•¸æ“šæ¨£æœ¬...")
            samples = self._generate_realistic_samples(100)
        
        self.processed_samples = samples
        self._compute_data_statistics()
        
        return samples
    
    def _categorize_difficulty(self, stroke_count: int) -> str:
        """åˆ†é¡é›£åº¦ç­‰ç´š"""
        if stroke_count <= 3:
            return "easy"
        elif stroke_count <= 8:
            return "medium"
        elif stroke_count <= 15:
            return "hard"
        else:
            return "very_hard"
    
    def _extract_features(self, item: Dict) -> np.ndarray:
        """æå–ç‰¹å¾µå‘é‡"""
        # åŸºæ–¼å­—ç¬¦ä¿¡æ¯ç”Ÿæˆç‰¹å¾µ
        features = np.zeros(20)
        
        # ç­†ç•«æ•¸ç‰¹å¾µ
        features[0] = item["stroke_count"] / 20.0
        
        # å­—ç¬¦è¤‡é›œåº¦ç‰¹å¾µ
        char = item["character"]
        features[1] = len(char.encode('utf-8')) / 10.0
        
        # ç­†ç•«ç‰¹å¾µï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if "strokes" in item:
            strokes = item["strokes"]
            features[2] = len(strokes) / 20.0
            
            # ç­†ç•«é•·åº¦ç‰¹å¾µ
            if strokes:
                avg_stroke_length = np.mean([len(stroke) for stroke in strokes])
                features[3] = min(1.0, avg_stroke_length / 100.0)
        
        # éš¨æ©Ÿç‰¹å¾µï¼ˆæ¨¡æ“¬å…¶ä»–å¾©é›œç‰¹å¾µï¼‰
        features[4:] = np.random.randn(16) * 0.1
        
        return features
    
    def _generate_realistic_samples(self, num_samples: int) -> List[Dict]:
        """ç”Ÿæˆé€¼çœŸçš„æ¨¡æ“¬æ¨£æœ¬"""
        samples = []
        chinese_chars = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åäººå¤§å°ä¸­å¤©åœ°ä¸Šä¸‹å·¦å³æ±è¥¿å—åŒ—æ˜¥å¤ç§‹å†¬å±±æ°´ç«æœ¨é‡‘åœŸæ—¥æœˆæ˜Ÿé›²é¢¨é›¨é›·é›»"
        
        for i in range(num_samples):
            char = random.choice(chinese_chars)
            stroke_count = random.randint(1, 20)
            
            sample = {
                "character": char,
                "stroke_count": stroke_count,
                "complexity": min(1.0, stroke_count / 20.0),
                "difficulty_level": self._categorize_difficulty(stroke_count),
                "feature_vector": np.random.randn(20) * 0.5 + 0.5
            }
            samples.append(sample)
        
        return samples
    
    def _compute_data_statistics(self):
        """è¨ˆç®—æ•¸æ“šçµ±è¨ˆ"""
        if not self.processed_samples:
            return
        
        stroke_counts = [s["stroke_count"] for s in self.processed_samples]
        complexities = [s["complexity"] for s in self.processed_samples]
        
        self.data_statistics = {
            "total_samples": len(self.processed_samples),
            "stroke_count": {
                "mean": np.mean(stroke_counts),
                "std": np.std(stroke_counts),
                "min": np.min(stroke_counts),
                "max": np.max(stroke_counts)
            },
            "complexity": {
                "mean": np.mean(complexities),
                "std": np.std(complexities),
                "min": np.min(complexities),
                "max": np.max(complexities)
            },
            "difficulty_distribution": {
                level: sum(1 for s in self.processed_samples if s["difficulty_level"] == level)
                for level in ["easy", "medium", "hard", "very_hard"]
            }
        }
        
        print(f"ğŸ“Š æ•¸æ“šçµ±è¨ˆ:")
        print(f"   ç¸½æ¨£æœ¬æ•¸: {self.data_statistics['total_samples']}")
        print(f"   å¹³å‡ç­†ç•«æ•¸: {self.data_statistics['stroke_count']['mean']:.1f}")
        print(f"   é›£åº¦åˆ†ä½ˆ: {self.data_statistics['difficulty_distribution']}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ === OCR0712 RLç­–ç•¥èª¿å„ªç³»çµ± ===")
    print()
    
    # å‰µå»ºé…ç½®
    tuning_config = RLTuningConfig(
        exploration_rate=0.15,
        learning_rate=5e-4,
        batch_size=64,
        adaptation_rate=0.02
    )
    
    # è¼‰å…¥çœŸå¯¦æ•¸æ“š
    data_integrator = RealDataIntegrator()
    real_data_samples = data_integrator.load_real_data_samples()
    
    # å‰µå»ºåƒæ•¸å„ªåŒ–å™¨
    optimizer = ParameterOptimizer(tuning_config)
    
    # åŸ·è¡Œè‡ªé©æ‡‰èª¿å„ª
    tuning_report = optimizer.adaptive_tuning(real_data_samples, num_iterations=15)
    
    # ä¿å­˜èª¿å„ªå ±å‘Š
    report_file = Path("rl_strategy_tuning_report.json")
    
    def convert_numpy_types(obj):
        if isinstance(obj, (np.integer, int)):
            return int(obj)
        elif isinstance(obj, (np.floating, float)):
            return float(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    tuning_report_serializable = convert_numpy_types(tuning_report)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(tuning_report_serializable, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š èª¿å„ªå ±å‘Šå·²ä¿å­˜: {report_file}")
    
    # é¡¯ç¤ºæœ€ä½³åƒæ•¸
    print(f"\nğŸ† === æœ€ä½³åƒæ•¸é…ç½® ===")
    for param_name, value in tuning_report["best_parameters"].items():
        if isinstance(value, np.ndarray):
            print(f"   {param_name}: [{', '.join(f'{v:.3f}' for v in value[:3])}...]")
        else:
            print(f"   {param_name}: {value:.3f}")
    
    # é¡¯ç¤ºå»ºè­°
    print(f"\nğŸ’¡ èª¿å„ªå»ºè­°:")
    for rec in tuning_report["recommendations"]:
        print(f"   â€¢ {rec}")

if __name__ == "__main__":
    main()