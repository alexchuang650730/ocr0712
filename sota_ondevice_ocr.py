#!/usr/bin/env python3
"""
SOTA On-Device OCR System
åŸºäº OCRFlux + GANè½¨è¿¹ + Scaling RL çš„è®¾å¤‡ç«¯OCRè§£å†³æ–¹æ¡ˆ
æ”¯æŒç¹ç®€ä¸­æ–‡åˆ†ç¦»å¤„ç†
"""

import numpy as np
import torch
import torch.nn as nn
import cv2
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import json
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ScriptType(Enum):
    """æ–‡å­—ç±»å‹æšä¸¾"""
    TRADITIONAL_CHINESE = "traditional_chinese"
    SIMPLIFIED_CHINESE = "simplified_chinese"
    ENGLISH = "english"
    JAPANESE = "japanese"
    KOREAN = "korean"
    ARABIC = "arabic"
    DIGITS = "digits"
    MIXED = "mixed"

@dataclass
class OCRResult:
    """OCRè¯†åˆ«ç»“æœ"""
    text: str
    confidence: float
    script_type: ScriptType
    trajectory_code: Optional[str] = None
    bounding_boxes: Optional[List[Tuple[int, int, int, int]]] = None
    processing_time: Optional[float] = None

@dataclass
class ContentAnalysis:
    """å†…å®¹åˆ†æç»“æœ"""
    script_type: ScriptType
    has_tables: bool = False
    has_handwriting: bool = False
    has_mixed_content: bool = False
    complexity_score: float = 0.0
    confidence: float = 0.0

class ChineseInertialGAN(nn.Module):
    """ä¸­æ–‡æƒ¯æ€§GAN - è½¨è¿¹ç”Ÿæˆæ¨¡å‹"""
    
    def __init__(self, script_type: ScriptType = ScriptType.SIMPLIFIED_CHINESE):
        super().__init__()
        self.script_type = script_type
        
        # ç¼–ç å™¨ - ä»å›¾åƒåˆ°ç‰¹å¾
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), 
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.Linear(256 * 8 * 8, 512)
        )
        
        # è½¨è¿¹ç”Ÿæˆå™¨
        self.trajectory_generator = nn.Sequential(
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Linear(2048, 1000)  # å‡è®¾æœ€å¤š500ä¸ªç‚¹ï¼Œæ¯ä¸ªç‚¹(x,y)
        )
        
        # æ ¹æ®æ–‡å­—ç±»å‹è°ƒæ•´å¤æ‚åº¦
        self.complexity_factor = 1.0 if script_type == ScriptType.SIMPLIFIED_CHINESE else 1.4
        
    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ç”Ÿæˆè½¨è¿¹"""
        features = self.encoder(image)
        trajectory = self.trajectory_generator(features)
        
        # æ ¹æ®ç¹ç®€ä¸­æ–‡è°ƒæ•´è½¨è¿¹å¤æ‚åº¦
        if self.script_type == ScriptType.TRADITIONAL_CHINESE:
            trajectory = trajectory * self.complexity_factor
            
        return trajectory.view(-1, 500, 2)  # é‡å¡‘ä¸ºåæ ‡ç‚¹åºåˆ—

class TrajectoryToCodeConverter:
    """è½¨è¿¹åˆ°ä»£ç è½¬æ¢å™¨"""
    
    def __init__(self):
        self.stroke_types = ['horizontal', 'vertical', 'dot', 'hook', 'bend']
        
    def convert_trajectory_to_code(self, trajectory: np.ndarray, script_type: ScriptType) -> str:
        """å°†è½¨è¿¹è½¬æ¢ä¸ºå¯æ‰§è¡Œä»£ç """
        
        # åˆ†æè½¨è¿¹ç‰¹å¾
        stroke_analysis = self._analyze_strokes(trajectory)
        
        # ç”Ÿæˆä»£ç 
        code = self._generate_character_code(stroke_analysis, script_type)
        
        return code
    
    def _analyze_strokes(self, trajectory: np.ndarray) -> Dict:
        """åˆ†æç¬”ç”»ç‰¹å¾"""
        
        # ç®€åŒ–çš„ç¬”ç”»åˆ†æ
        strokes = []
        for i in range(0, len(trajectory)-1, 10):  # æ¯10ä¸ªç‚¹ä½œä¸ºä¸€ä¸ªç¬”ç”»æ®µ
            segment = trajectory[i:i+10]
            stroke_type = self._classify_stroke(segment)
            strokes.append({
                'type': stroke_type,
                'coordinates': segment.tolist(),
                'length': np.sum(np.diff(segment, axis=0)**2)**0.5
            })
        
        return {'strokes': strokes, 'total_strokes': len(strokes)}
    
    def _classify_stroke(self, segment: np.ndarray) -> str:
        """åˆ†ç±»ç¬”ç”»ç±»å‹"""
        if len(segment) < 2:
            return 'dot'
            
        # ç®€åŒ–çš„ç¬”ç”»åˆ†ç±»é€»è¾‘
        dx = segment[-1][0] - segment[0][0]
        dy = segment[-1][1] - segment[0][1]
        
        if abs(dx) > abs(dy) * 2:
            return 'horizontal'
        elif abs(dy) > abs(dx) * 2:
            return 'vertical'
        else:
            return 'bend'
    
    def _generate_character_code(self, stroke_analysis: Dict, script_type: ScriptType) -> str:
        """ç”Ÿæˆå­—ç¬¦ä»£ç """
        
        code_lines = [
            f"# Generated code for {script_type.value} character",
            "def draw_character():",
            f"    strokes = {stroke_analysis['strokes']}",
            f"    total_strokes = {stroke_analysis['total_strokes']}",
            "    ",
            "    # Render character based on stroke data",
            "    character = CharacterRenderer(strokes)",
            "    return character.render()",
            ""
        ]
        
        return "\n".join(code_lines)

class MockOCRFlux3B:
    """æ¨¡æ‹ŸOCRFlux 3Bæ¨¡å‹ (å®é™…éƒ¨ç½²æ—¶æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹)"""
    
    def __init__(self):
        self.model_loaded = True
        logger.info("OCRFlux 3B model initialized (mock)")
        
    def process_table_structure(self, image: np.ndarray) -> Dict:
        """å¤„ç†è¡¨æ ¼ç»“æ„"""
        
        # æ¨¡æ‹Ÿè¡¨æ ¼è¯†åˆ«ç»“æœ
        mock_result = {
            'tables': [
                {
                    'bbox': [50, 50, 400, 300],
                    'cells': [
                        {'text': 'å§“å', 'bbox': [50, 50, 150, 100]},
                        {'text': 'å¹´é¾„', 'bbox': [150, 50, 250, 100]},
                        {'text': 'å¼ ä¸‰', 'bbox': [50, 100, 150, 150]},
                        {'text': '25', 'bbox': [150, 100, 250, 150]}
                    ]
                }
            ],
            'confidence': 0.92,
            'processing_time': 0.15
        }
        
        return mock_result
    
    def process_document_layout(self, image: np.ndarray) -> Dict:
        """å¤„ç†æ–‡æ¡£å¸ƒå±€"""
        
        mock_result = {
            'layout': {
                'text_regions': [[100, 100, 400, 200]],
                'table_regions': [[50, 250, 450, 400]], 
                'figure_regions': []
            },
            'reading_order': [0, 1],  # å…ˆæ–‡æœ¬åè¡¨æ ¼
            'confidence': 0.89
        }
        
        return mock_result

class ScriptTypeDetector:
    """æ–‡å­—ç±»å‹æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.traditional_chars = set('ç¹é«”ä¸­æ–‡å­—ç¬¦ç¯„ä¾‹')  # ç®€åŒ–ç¤ºä¾‹
        self.simplified_chars = set('ç®€ä½“ä¸­æ–‡å­—ç¬¦èŒƒä¾‹')   # ç®€åŒ–ç¤ºä¾‹
        
    def detect_script_type(self, image: np.ndarray) -> ContentAnalysis:
        """æ£€æµ‹æ–‡å­—ç±»å‹"""
        
        # ç®€åŒ–çš„æ£€æµ‹é€»è¾‘
        analysis = ContentAnalysis(script_type=ScriptType.SIMPLIFIED_CHINESE)
        
        # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
        height, width = image.shape[:2]
        
        # åŸºäºå›¾åƒç‰¹å¾çš„ç®€å•åˆ¤æ–­
        if width > height * 2:  # å®½å›¾å¯èƒ½æ˜¯è¡¨æ ¼
            analysis.has_tables = True
            
        # æ£€æµ‹æ‰‹å†™ç‰¹å¾ (ç®€åŒ–)
        if self._detect_handwriting_features(image):
            analysis.has_handwriting = True
            
        # æ£€æµ‹ä¸­æ–‡ç±»å‹ (è¿™é‡Œç®€åŒ–ä¸ºéšæœº,å®é™…éœ€è¦å­—ç¬¦è¯†åˆ«)
        if np.random.random() > 0.5:
            analysis.script_type = ScriptType.TRADITIONAL_CHINESE
        else:
            analysis.script_type = ScriptType.SIMPLIFIED_CHINESE
            
        analysis.confidence = 0.85
        analysis.complexity_score = 0.7
        
        return analysis
    
    def _detect_handwriting_features(self, image: np.ndarray) -> bool:
        """æ£€æµ‹æ‰‹å†™ç‰¹å¾"""
        
        # ç®€åŒ–çš„æ‰‹å†™æ£€æµ‹ - åŸºäºè¾¹ç¼˜ä¸è§„åˆ™æ€§
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        edges = cv2.Canny(gray, 50, 150)
        
        # è®¡ç®—è¾¹ç¼˜çš„ä¸è§„åˆ™æ€§
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if len(contours) > 0:
            # ç®€å•çš„ä¸è§„åˆ™æ€§åº¦é‡
            total_irregularity = 0
            for contour in contours:
                if len(contour) > 10:
                    hull = cv2.convexHull(contour)
                    hull_area = cv2.contourArea(hull)
                    contour_area = cv2.contourArea(contour)
                    if hull_area > 0:
                        irregularity = 1 - (contour_area / hull_area)
                        total_irregularity += irregularity
            
            avg_irregularity = total_irregularity / len(contours)
            return avg_irregularity > 0.3  # é˜ˆå€¼å¯è°ƒ
            
        return False

class ScalingRLOptimizer:
    """Scaling RLä¼˜åŒ–å™¨ (åŸºäºDeepSWEæ–¹æ³•)"""
    
    def __init__(self):
        self.optimization_steps = 0
        self.max_steps = 100
        
        # DeepSWEçš„7ä¸ªå…³é”®ä¼˜åŒ–
        self.optimizations = {
            'clip_high_dapo': True,
            'remove_kl_loss': True,
            'remove_reward_std': True, 
            'length_normalization': True,
            'one_sample_removal': True,
            'compact_filtering': True,
            'remove_entropy_loss': True
        }
        
    def optimize_recognition(self, base_result: str, image: np.ndarray) -> str:
        """ä½¿ç”¨RLä¼˜åŒ–è¯†åˆ«ç»“æœ"""
        
        # æ¨¡æ‹ŸRLä¼˜åŒ–è¿‡ç¨‹
        optimized_result = base_result
        
        for step in range(min(10, self.max_steps)):  # é™åˆ¶æ­¥æ•°ä»¥æé«˜é€Ÿåº¦
            # æ¨¡æ‹Ÿç­–ç•¥æ”¹è¿›
            improvement = self._apply_rl_step(optimized_result, image)
            if improvement:
                optimized_result = improvement
                
        logger.info(f"RL optimization completed in {step+1} steps")
        return optimized_result
    
    def _apply_rl_step(self, current_result: str, image: np.ndarray) -> Optional[str]:
        """åº”ç”¨å•æ­¥RLä¼˜åŒ–"""
        
        # ç®€åŒ–çš„RLæ­¥éª¤ - å®é™…ä¸­è¿™é‡Œä¼šæœ‰å¤æ‚çš„ç¥ç»ç½‘ç»œ
        if np.random.random() > 0.7:  # 30%æ¦‚ç‡æ”¹è¿›
            # æ¨¡æ‹Ÿæ”¹è¿› - æ¯”å¦‚çº æ­£å¸¸è§é”™è¯¯
            corrections = {
                '0': 'O',  # æ•°å­—0 -> å­—æ¯O
                '1': 'l',  # æ•°å­—1 -> å°å†™L  
                '5': 'S',  # æ•°å­—5 -> å­—æ¯S
            }
            
            for old, new in corrections.items():
                if old in current_result:
                    return current_result.replace(old, new, 1)  # åªæ›¿æ¢ç¬¬ä¸€ä¸ª
                    
        return None

class SOTAOnDeviceOCR:
    """SOTAçº§è®¾å¤‡ç«¯OCRä¸»ç³»ç»Ÿ"""
    
    def __init__(self):
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.script_detector = ScriptTypeDetector()
        self.ocrflux_engine = MockOCRFlux3B()
        
        # GANæ¨¡å‹å­—å…¸ - ä¸ºä¸åŒæ–‡å­—ç±»å‹å‡†å¤‡ä¸åŒæ¨¡å‹
        self.gan_models = {
            ScriptType.TRADITIONAL_CHINESE: ChineseInertialGAN(ScriptType.TRADITIONAL_CHINESE),
            ScriptType.SIMPLIFIED_CHINESE: ChineseInertialGAN(ScriptType.SIMPLIFIED_CHINESE),
        }
        
        self.trajectory_converter = TrajectoryToCodeConverter()
        self.rl_optimizer = ScalingRLOptimizer()
        
        logger.info("SOTA On-Device OCR System initialized")
    
    def recognize(self, image_path: str) -> OCRResult:
        """ä¸»è¦è¯†åˆ«æ¥å£"""
        
        import time
        start_time = time.time()
        
        # 1. åŠ è½½å›¾åƒ
        image = self._load_image(image_path)
        if image is None:
            return OCRResult("Error: Could not load image", 0.0, ScriptType.MIXED)
        
        # 2. åˆ†æå†…å®¹ç±»å‹
        analysis = self.script_detector.detect_script_type(image)
        logger.info(f"Detected script type: {analysis.script_type}")
        
        # 3. æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
        if analysis.has_tables:
            # ä½¿ç”¨OCRFluxå¤„ç†è¡¨æ ¼
            result = self._process_with_ocrflux(image, analysis)
        elif analysis.has_handwriting:
            # ä½¿ç”¨GAN+RLå¤„ç†æ‰‹å†™
            result = self._process_with_gan_rl(image, analysis)
        else:
            # æ··åˆå¤„ç†
            result = self._process_mixed_content(image, analysis)
        
        # 4. æœ€ç»ˆä¼˜åŒ–
        if result.trajectory_code:
            optimized_text = self.rl_optimizer.optimize_recognition(result.text, image)
            result.text = optimized_text
        
        result.processing_time = time.time() - start_time
        logger.info(f"Recognition completed in {result.processing_time:.2f}s")
        
        return result
    
    def _load_image(self, image_path: str) -> Optional[np.ndarray]:
        """åŠ è½½å›¾åƒ"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                logger.error(f"Failed to load image: {image_path}")
                return None
            return image
        except Exception as e:
            logger.error(f"Error loading image: {e}")
            return None
    
    def _process_with_ocrflux(self, image: np.ndarray, analysis: ContentAnalysis) -> OCRResult:
        """ä½¿ç”¨OCRFluxå¤„ç†è¡¨æ ¼å’Œç»“æ„åŒ–å†…å®¹"""
        
        table_result = self.ocrflux_engine.process_table_structure(image)
        layout_result = self.ocrflux_engine.process_document_layout(image)
        
        # æå–è¡¨æ ¼æ–‡æœ¬
        extracted_text = ""
        bboxes = []
        
        for table in table_result.get('tables', []):
            for cell in table.get('cells', []):
                extracted_text += cell['text'] + " "
                bboxes.append(tuple(cell['bbox']))
        
        return OCRResult(
            text=extracted_text.strip(),
            confidence=table_result.get('confidence', 0.8),
            script_type=analysis.script_type,
            bounding_boxes=bboxes
        )
    
    def _process_with_gan_rl(self, image: np.ndarray, analysis: ContentAnalysis) -> OCRResult:
        """ä½¿ç”¨GAN+RLå¤„ç†æ‰‹å†™å†…å®¹"""
        
        # é€‰æ‹©åˆé€‚çš„GANæ¨¡å‹
        gan_model = self.gan_models.get(
            analysis.script_type, 
            self.gan_models[ScriptType.SIMPLIFIED_CHINESE]
        )
        
        # é¢„å¤„ç†å›¾åƒ
        processed_image = self._preprocess_for_gan(image)
        
        # GANç”Ÿæˆè½¨è¿¹
        with torch.no_grad():
            trajectory = gan_model(processed_image)
            trajectory_np = trajectory.cpu().numpy()[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        # è½¨è¿¹è½¬ä»£ç 
        trajectory_code = self.trajectory_converter.convert_trajectory_to_code(
            trajectory_np, analysis.script_type
        )
        
        # ä»è½¨è¿¹æ¨æ–­æ–‡å­— (ç®€åŒ–)
        recognized_text = self._trajectory_to_text(trajectory_np, analysis.script_type)
        
        return OCRResult(
            text=recognized_text,
            confidence=0.88,
            script_type=analysis.script_type,
            trajectory_code=trajectory_code
        )
    
    def _process_mixed_content(self, image: np.ndarray, analysis: ContentAnalysis) -> OCRResult:
        """å¤„ç†æ··åˆå†…å®¹"""
        
        # ç®€åŒ–çš„æ··åˆå¤„ç† - ç»„åˆä¸¤ç§æ–¹æ³•çš„ç»“æœ
        table_result = self._process_with_ocrflux(image, analysis)
        handwriting_result = self._process_with_gan_rl(image, analysis)
        
        # åˆå¹¶ç»“æœ
        combined_text = f"{table_result.text} {handwriting_result.text}"
        combined_confidence = (table_result.confidence + handwriting_result.confidence) / 2
        
        return OCRResult(
            text=combined_text.strip(),
            confidence=combined_confidence,
            script_type=analysis.script_type,
            trajectory_code=handwriting_result.trajectory_code,
            bounding_boxes=table_result.bounding_boxes
        )
    
    def _preprocess_for_gan(self, image: np.ndarray) -> torch.Tensor:
        """ä¸ºGANé¢„å¤„ç†å›¾åƒ"""
        
        # è½¬ç°åº¦
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # è°ƒæ•´å¤§å°
        resized = cv2.resize(gray, (64, 64))
        
        # æ ‡å‡†åŒ–
        normalized = resized.astype(np.float32) / 255.0
        
        # è½¬ä¸ºtensor
        tensor = torch.from_numpy(normalized).unsqueeze(0).unsqueeze(0)  # æ·»åŠ batchå’Œchannelç»´åº¦
        
        return tensor
    
    def _trajectory_to_text(self, trajectory: np.ndarray, script_type: ScriptType) -> str:
        """ä»è½¨è¿¹æ¨æ–­æ–‡å­—å†…å®¹ (ç®€åŒ–å®ç°)"""
        
        # è¿™é‡Œæ˜¯ç®€åŒ–çš„å®ç°ï¼Œå®é™…ä¸­éœ€è¦å¤æ‚çš„è½¨è¿¹åˆ†æ
        stroke_count = len(trajectory) // 10  # ç®€å•çš„ç¬”ç”»è®¡æ•°
        
        if script_type == ScriptType.TRADITIONAL_CHINESE:
            if stroke_count > 15:
                return "è¤‡é›œç¹é«”å­—"
            elif stroke_count > 8:
                return "ä¸­ç­‰ç¹é«”å­—"
            else:
                return "ç°¡å–®ç¹é«”å­—"
        elif script_type == ScriptType.SIMPLIFIED_CHINESE:
            if stroke_count > 12:
                return "å¤æ‚ç®€ä½“å­—"
            elif stroke_count > 6:
                return "ä¸­ç­‰ç®€ä½“å­—"
            else:
                return "ç®€å•ç®€ä½“å­—"
        else:
            return f"è¯†åˆ«æ–‡å­— (ç¬”ç”»æ•°: {stroke_count})"

# æµ‹è¯•æ¥å£
class OCRTester:
    """OCRæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.ocr_system = SOTAOnDeviceOCR()
        
    def test_single_image(self, image_path: str) -> None:
        """æµ‹è¯•å•å¼ å›¾åƒ"""
        
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•å›¾åƒ: {image_path}")
        print(f"{'='*60}")
        
        result = self.ocr_system.recognize(image_path)
        
        print(f"è¯†åˆ«ç»“æœ: {result.text}")
        print(f"æ–‡å­—ç±»å‹: {result.script_type.value}")
        print(f"ç½®ä¿¡åº¦: {result.confidence:.3f}")
        print(f"å¤„ç†æ—¶é—´: {result.processing_time:.3f}ç§’")
        
        if result.trajectory_code:
            print(f"\nç”Ÿæˆçš„è½¨è¿¹ä»£ç :")
            print(result.trajectory_code)
            
        if result.bounding_boxes:
            print(f"\nè¾¹ç•Œæ¡†æ•°é‡: {len(result.bounding_boxes)}")
            
        print(f"{'='*60}")
    
    def create_test_image(self, filename: str, content_type: str = "simple") -> str:
        """åˆ›å»ºæµ‹è¯•å›¾åƒ"""
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•å›¾åƒ
        if content_type == "simple":
            image = np.ones((200, 300, 3), dtype=np.uint8) * 255
            cv2.putText(image, "Test Image", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        elif content_type == "table":
            image = np.ones((400, 600, 3), dtype=np.uint8) * 255
            # ç”»ç®€å•è¡¨æ ¼
            cv2.rectangle(image, (50, 50), (550, 350), (0, 0, 0), 2)
            cv2.line(image, (300, 50), (300, 350), (0, 0, 0), 1)
            cv2.line(image, (50, 200), (550, 200), (0, 0, 0), 1)
            cv2.putText(image, "Name", (80, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
            cv2.putText(image, "Age", (350, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
        elif content_type == "handwriting":
            image = np.ones((300, 400, 3), dtype=np.uint8) * 255
            # æ¨¡æ‹Ÿæ‰‹å†™é£æ ¼ (ä¸è§„åˆ™çº¿æ¡)
            points = np.array([[100, 100], [150, 120], [200, 110], [250, 130]], np.int32)
            cv2.polylines(image, [points], False, (0, 0, 0), 3)
        
        cv2.imwrite(filename, image)
        return filename

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç³»ç»ŸåŠŸèƒ½"""
    
    print("ğŸš€ SOTA On-Device OCR ç³»ç»Ÿå¯åŠ¨")
    print("åŸºäº OCRFlux + GANè½¨è¿¹ + Scaling RL")
    print("æ”¯æŒç¹ç®€ä¸­æ–‡åˆ†ç¦»å¤„ç†\n")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = OCRTester()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_images = {
        "simple_test.jpg": "simple",
        "table_test.jpg": "table", 
        "handwriting_test.jpg": "handwriting"
    }
    
    print("åˆ›å»ºæµ‹è¯•å›¾åƒ...")
    for filename, content_type in test_images.items():
        tester.create_test_image(filename, content_type)
        print(f"âœ“ åˆ›å»º {filename} ({content_type})")
    
    print("\nå¼€å§‹OCRæµ‹è¯•...")
    
    # æµ‹è¯•æ¯å¼ å›¾åƒ
    for filename in test_images.keys():
        try:
            tester.test_single_image(filename)
        except Exception as e:
            print(f"æµ‹è¯• {filename} æ—¶å‡ºé”™: {e}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print("1. å°†æ‚¨çš„æµ‹è¯•å›¾åƒæ”¾åœ¨å½“å‰ç›®å½•")
    print("2. è°ƒç”¨ tester.test_single_image('your_image.jpg')")
    print("3. ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å†…å®¹ç±»å‹å¹¶é€‰æ‹©æœ€ä½³å¤„ç†ç­–ç•¥")

if __name__ == "__main__":
    main()