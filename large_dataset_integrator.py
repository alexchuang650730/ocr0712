#!/usr/bin/env python3
"""
OCR0712 å¤§å‹æ•¸æ“šé›†æ•´åˆå™¨
æ•´åˆCASIA-HWDBã€HIT-MWã€SCUT-EPTç­‰å­¸è¡“ç´šä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†
"""

import os
import sys
import json
import time
import requests
import zipfile
import tarfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import hashlib
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

class LargeDatasetIntegrator:
    """å¤§å‹æ•¸æ“šé›†æ•´åˆå™¨"""
    
    def __init__(self, base_dir: str = "./large_datasets"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        # å¤§å‹æ•¸æ“šé›†é…ç½®
        self.dataset_configs = {
            "casia_hwdb": {
                "name": "CASIA-HWDB2.0/2.1/2.2",
                "description": "ä¸­ç§‘é™¢è‡ªå‹•åŒ–æ‰€ä¸­æ–‡æ‰‹å¯«æ•¸æ“šåº«",
                "size_estimate": "10GB+",
                "character_count": "7000+ å­—ç¬¦",
                "sample_count": "1.2M+ æ¨£æœ¬",
                "urls": [
                    "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB2.0Train.zip",
                    "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB2.1Train.zip", 
                    "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB2.2Train.zip"
                ],
                "backup_info": "éœ€è¦å­¸è¡“æ©Ÿæ§‹ç”³è«‹è¨±å¯",
                "format": "Isolated Character Dataset"
            },
            
            "hit_mw": {
                "name": "HIT-MW (Harbin Institute of Technology)",
                "description": "å“ˆå·¥å¤§ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†",
                "size_estimate": "2GB+",
                "character_count": "3000+ å­—ç¬¦",
                "sample_count": "200K+ æ¨£æœ¬",
                "urls": [
                    "https://github.com/hitcszj/HIT-MW",
                    "https://dataset.hitcszj.com/HIT-MW/HIT-MW.zip"
                ],
                "backup_info": "é–‹æºæ•¸æ“šé›†ï¼ŒGitHubå¯ç²å–",
                "format": "Word-level Dataset"
            },
            
            "scut_ept": {
                "name": "SCUT-EPT",
                "description": "è¯å—ç†å·¥English and Chinese text dataset",
                "size_estimate": "1.5GB+",
                "character_count": "2000+ å­—ç¬¦",
                "sample_count": "100K+ æ¨£æœ¬",
                "urls": [
                    "https://github.com/HCIILAB/SCUT-EPT_Dataset_Release",
                    "https://pan.baidu.com/s/1QkI7kjah8SPHwOQ40rS6Pw"
                ],
                "backup_info": "ç™¾åº¦ç¶²ç›¤: code: scut",
                "format": "English+Chinese mixed"
            },
            
            "chn_handwriting": {
                "name": "Chinese Handwriting Dataset Collection",
                "description": "å¤šä¾†æºä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†åˆ",
                "size_estimate": "5GB+",
                "character_count": "5000+ å­—ç¬¦",
                "sample_count": "500K+ æ¨£æœ¬",
                "urls": [
                    "https://www.kaggle.com/datasets/pascalbliem/handwritten-chinese-character-hanzi",
                    "https://github.com/skishore/makemeahanzi/releases/download/1.0/graphics.txt",
                    "https://github.com/chanind/hanzi-writer-data"
                ],
                "backup_info": "Kaggle + GitHubé–‹æº",
                "format": "Multiple formats"
            },
            
            "calamari_chinese": {
                "name": "Calamari Chinese OCR Dataset",
                "description": "Calamari OCRæ¡†æ¶ä¸­æ–‡æ•¸æ“šé›†",
                "size_estimate": "800MB+",
                "character_count": "4000+ å­—ç¬¦", 
                "sample_count": "50K+ æ¨£æœ¬",
                "urls": [
                    "https://github.com/Calamari-OCR/calamari_models/tree/master/antiqua_historical",
                    "https://zenodo.org/record/4428683/files/chinese_traditional.zip"
                ],
                "backup_info": "Zenodoå­¸è¡“å¹³å°",
                "format": "OCR-ready format"
            }
        }
        
        # ä¸‹è¼‰çµ±è¨ˆ
        self.download_stats = {
            "total_datasets": len(self.dataset_configs),
            "attempted_downloads": 0,
            "successful_downloads": 0,
            "failed_downloads": 0,
            "total_size_downloaded": 0,
            "estimated_character_count": 0,
            "estimated_sample_count": 0
        }
        
        print(f"ğŸ—„ï¸ === OCR0712 å¤§å‹æ•¸æ“šé›†æ•´åˆå™¨ ===")
        print(f"ğŸ“Š ç›®æ¨™æ•¸æ“šé›†: {len(self.dataset_configs)} å€‹")
        print(f"ğŸ“ åŸºç¤ç›®éŒ„: {self.base_dir.absolute()}")
        print()
    
    def analyze_dataset_requirements(self):
        """åˆ†ææ•¸æ“šé›†éœ€æ±‚"""
        print("ğŸ“‹ === æ•¸æ“šé›†éœ€æ±‚åˆ†æ ===")
        
        total_size_estimate = 0
        total_characters = 0
        total_samples = 0
        
        for dataset_id, config in self.dataset_configs.items():
            print(f"\nğŸ“¦ {config['name']}")
            print(f"   æè¿°: {config['description']}")
            print(f"   ä¼°è¨ˆå¤§å°: {config['size_estimate']}")
            print(f"   å­—ç¬¦æ•¸: {config['character_count']}")
            print(f"   æ¨£æœ¬æ•¸: {config['sample_count']}")
            print(f"   æ ¼å¼: {config['format']}")
            print(f"   ç²å–èªªæ˜: {config['backup_info']}")
            
            # è§£ææ•¸å€¼
            size_str = config['size_estimate'].replace('GB+', '').replace('MB+', '')
            if 'GB' in config['size_estimate']:
                size_gb = float(size_str)
                total_size_estimate += size_gb
            elif 'MB' in config['size_estimate']:
                size_mb = float(size_str) / 1024
                total_size_estimate += size_mb
            
            char_count_str = config['character_count'].split('+')[0].replace('K', '000').replace(' å­—ç¬¦', '')
            char_count = int(char_count_str)
            
            sample_count_str = config['sample_count'].replace('K+', '000').replace('M+', '000000').replace('+', '').replace(' æ¨£æœ¬', '')
            sample_count = int(float(sample_count_str.split('.')[0]))
            
            total_characters += char_count
            total_samples += sample_count
        
        print(f"\nğŸ“Š === ç¸½è¨ˆéœ€æ±‚ ===")
        print(f"   ç¸½ä¼°è¨ˆå¤§å°: {total_size_estimate:.1f} GB")
        print(f"   ç¸½å­—ç¬¦æ•¸: {total_characters:,} å€‹")
        print(f"   ç¸½æ¨£æœ¬æ•¸: {total_samples:,} å€‹")
        print(f"   å­˜å„²ç©ºé–“éœ€æ±‚: {total_size_estimate * 1.5:.1f} GB (å«è™•ç†ç©ºé–“)")
        
        return {
            "total_size_gb": total_size_estimate,
            "total_characters": total_characters,
            "total_samples": total_samples
        }
    
    def create_dataset_download_plan(self):
        """å‰µå»ºæ•¸æ“šé›†ä¸‹è¼‰è¨ˆåŠƒ"""
        print("\nğŸ—ºï¸ === æ•¸æ“šé›†ä¸‹è¼‰è¨ˆåŠƒ ===")
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        priority_order = [
            ("chn_handwriting", "é–‹æºå¯ç›´æ¥ä¸‹è¼‰"),
            ("hit_mw", "GitHubé–‹æº"),
            ("calamari_chinese", "å­¸è¡“å¹³å°å¯ç²å–"),
            ("scut_ept", "éœ€è¦ç™¾åº¦ç¶²ç›¤"),
            ("casia_hwdb", "éœ€è¦å­¸è¡“ç”³è«‹")
        ]
        
        download_plan = {
            "immediate_download": [],  # å¯ç«‹å³ä¸‹è¼‰
            "manual_download": [],     # éœ€è¦æ‰‹å‹•ç²å–
            "academic_request": []     # éœ€è¦å­¸è¡“ç”³è«‹
        }
        
        for dataset_id, reason in priority_order:
            config = self.dataset_configs[dataset_id]
            
            if "é–‹æº" in reason or "GitHub" in reason:
                download_plan["immediate_download"].append({
                    "id": dataset_id,
                    "config": config,
                    "reason": reason
                })
            elif "ç™¾åº¦ç¶²ç›¤" in reason or "å­¸è¡“å¹³å°" in reason:
                download_plan["manual_download"].append({
                    "id": dataset_id,
                    "config": config,
                    "reason": reason
                })
            else:
                download_plan["academic_request"].append({
                    "id": dataset_id,
                    "config": config,
                    "reason": reason
                })
        
        # é¡¯ç¤ºè¨ˆåŠƒ
        print("ğŸš€ ç«‹å³å¯ä¸‹è¼‰æ•¸æ“šé›†:")
        for item in download_plan["immediate_download"]:
            print(f"   âœ… {item['config']['name']} ({item['reason']})")
        
        print("\nğŸ“¥ éœ€è¦æ‰‹å‹•ç²å–æ•¸æ“šé›†:")
        for item in download_plan["manual_download"]:
            print(f"   ğŸ”„ {item['config']['name']} ({item['reason']})")
        
        print("\nğŸ“‹ éœ€è¦å­¸è¡“ç”³è«‹æ•¸æ“šé›†:")
        for item in download_plan["academic_request"]:
            print(f"   ğŸ“„ {item['config']['name']} ({item['reason']})")
        
        return download_plan
    
    def download_available_datasets(self, download_plan: Dict):
        """ä¸‹è¼‰å¯ç²å–çš„æ•¸æ“šé›†"""
        print("\nğŸ“¥ === é–‹å§‹ä¸‹è¼‰å¯ç²å–æ•¸æ“šé›† ===")
        
        for item in download_plan["immediate_download"]:
            dataset_id = item["id"]
            config = item["config"]
            
            print(f"\nğŸ“¦ æ­£åœ¨ä¸‹è¼‰: {config['name']}")
            
            dataset_dir = self.base_dir / dataset_id
            dataset_dir.mkdir(exist_ok=True)
            
            success_count = 0
            
            for i, url in enumerate(config["urls"]):
                try:
                    print(f"   ğŸ”— URL {i+1}: {url}")
                    
                    # æª¢æŸ¥URLé¡å‹
                    if "github.com" in url and not url.endswith(('.zip', '.tar.gz', '.txt')):
                        # GitHubå€‰åº«ï¼Œå˜—è©¦ç²å–release
                        self._download_github_repo(url, dataset_dir, dataset_id)
                    elif url.endswith('.txt'):
                        # æ–‡æœ¬æ–‡ä»¶ç›´æ¥ä¸‹è¼‰
                        self._download_text_file(url, dataset_dir, f"{dataset_id}_data.txt")
                    elif url.endswith(('.zip', '.tar.gz')):
                        # å£“ç¸®æ–‡ä»¶ä¸‹è¼‰
                        self._download_compressed_file(url, dataset_dir, dataset_id)
                    else:
                        # å˜—è©¦ä¸€èˆ¬ä¸‹è¼‰
                        self._download_general_file(url, dataset_dir, dataset_id, i)
                    
                    success_count += 1
                    print(f"   âœ… ä¸‹è¼‰æˆåŠŸ")
                    
                except Exception as e:
                    print(f"   âŒ ä¸‹è¼‰å¤±æ•—: {e}")
                    continue
            
            if success_count > 0:
                self.download_stats["successful_downloads"] += 1
                print(f"âœ… {config['name']} ä¸‹è¼‰å®Œæˆ ({success_count}/{len(config['urls'])} æˆåŠŸ)")
            else:
                self.download_stats["failed_downloads"] += 1
                print(f"âŒ {config['name']} ä¸‹è¼‰å¤±æ•—")
            
            self.download_stats["attempted_downloads"] += 1
    
    def _download_text_file(self, url: str, dataset_dir: Path, filename: str):
        """ä¸‹è¼‰æ–‡æœ¬æ–‡ä»¶"""
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            file_path = dataset_dir / filename
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            file_size = file_path.stat().st_size
            print(f"   ğŸ“„ å·²ä¸‹è¼‰: {filename} ({file_size:,} bytes)")
            self.download_stats["total_size_downloaded"] += file_size
            
        except Exception as e:
            raise Exception(f"æ–‡æœ¬æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {e}")
    
    def _download_compressed_file(self, url: str, dataset_dir: Path, dataset_id: str):
        """ä¸‹è¼‰å£“ç¸®æ–‡ä»¶"""
        try:
            filename = url.split('/')[-1]
            file_path = dataset_dir / filename
            
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            file_size = file_path.stat().st_size
            print(f"   ğŸ“¦ å·²ä¸‹è¼‰: {filename} ({file_size:,} bytes)")
            self.download_stats["total_size_downloaded"] += file_size
            
            # å¦‚æœæ˜¯å°æ–‡ä»¶ï¼Œå˜—è©¦è§£å£“
            if file_size < 100 * 1024 * 1024:  # å°æ–¼100MB
                self._extract_compressed_file(file_path, dataset_dir)
            
        except Exception as e:
            raise Exception(f"å£“ç¸®æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {e}")
    
    def _download_github_repo(self, url: str, dataset_dir: Path, dataset_id: str):
        """ä¸‹è¼‰GitHubå€‰åº«"""
        try:
            # å˜—è©¦ç²å–releaseä¿¡æ¯
            if "/tree/" in url:
                # ç‰¹å®šåˆ†æ”¯æˆ–è·¯å¾‘
                print(f"   ğŸ“‚ GitHubè·¯å¾‘: {url}")
                # å‰µå»ºèªªæ˜æ–‡ä»¶
                info_file = dataset_dir / f"{dataset_id}_github_info.txt"
                with open(info_file, 'w', encoding='utf-8') as f:
                    f.write(f"GitHub Repository: {url}\n")
                    f.write(f"Download Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"Instructions: Manual clone required\n")
                    f.write(f"Command: git clone {url.split('/tree/')[0]}\n")
            else:
                # ä¸»å€‰åº«
                repo_url = url.replace("github.com", "api.github.com/repos")
                if not repo_url.endswith('/releases/latest'):
                    repo_url += '/releases/latest'
                
                response = requests.get(repo_url, timeout=30)
                if response.status_code == 200:
                    release_data = response.json()
                    download_url = release_data.get('zipball_url')
                    if download_url:
                        self._download_compressed_file(download_url, dataset_dir, dataset_id)
                    else:
                        raise Exception("No downloadable release found")
                else:
                    raise Exception(f"GitHub API error: {response.status_code}")
            
        except Exception as e:
            raise Exception(f"GitHubä¸‹è¼‰å¤±æ•—: {e}")
    
    def _download_general_file(self, url: str, dataset_dir: Path, dataset_id: str, index: int):
        """ä¸€èˆ¬æ–‡ä»¶ä¸‹è¼‰"""
        try:
            response = requests.head(url, timeout=30)
            content_length = response.headers.get('content-length')
            
            if content_length and int(content_length) > 500 * 1024 * 1024:  # å¤§æ–¼500MB
                print(f"   âš ï¸  å¤§æ–‡ä»¶({int(content_length)/(1024*1024):.1f}MB)ï¼Œè·³éè‡ªå‹•ä¸‹è¼‰")
                # å‰µå»ºä¸‹è¼‰èªªæ˜
                info_file = dataset_dir / f"{dataset_id}_download_info_{index}.txt"
                with open(info_file, 'w', encoding='utf-8') as f:
                    f.write(f"Large File URL: {url}\n")
                    f.write(f"Size: {int(content_length)/(1024*1024):.1f} MB\n")
                    f.write(f"Manual download required\n")
                return
            
            # å°æ–‡ä»¶ç›´æ¥ä¸‹è¼‰
            response = requests.get(url, timeout=60)
            response.raise_for_status()
            
            filename = f"{dataset_id}_data_{index}.bin"
            file_path = dataset_dir / filename
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            file_size = len(response.content)
            print(f"   ğŸ“„ å·²ä¸‹è¼‰: {filename} ({file_size:,} bytes)")
            self.download_stats["total_size_downloaded"] += file_size
            
        except Exception as e:
            raise Exception(f"ä¸€èˆ¬ä¸‹è¼‰å¤±æ•—: {e}")
    
    def _extract_compressed_file(self, file_path: Path, extract_dir: Path):
        """è§£å£“ç¸®æ–‡ä»¶"""
        try:
            if file_path.suffix == '.zip':
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                print(f"   ğŸ“‚ å·²è§£å£“: {file_path.name}")
            elif file_path.suffix in ['.tar', '.gz']:
                with tarfile.open(file_path, 'r:*') as tar_ref:
                    tar_ref.extractall(extract_dir)
                print(f"   ğŸ“‚ å·²è§£å£“: {file_path.name}")
        except Exception as e:
            print(f"   âš ï¸  è§£å£“å¤±æ•—: {e}")
    
    def create_unified_dataset_index(self):
        """å‰µå»ºçµ±ä¸€æ•¸æ“šé›†ç´¢å¼•"""
        print("\nğŸ“‡ === å‰µå»ºçµ±ä¸€æ•¸æ“šé›†ç´¢å¼• ===")
        
        index_data = {
            "metadata": {
                "creation_time": time.time(),
                "creation_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "ocr0712_version": "v1.0 - Large Dataset Integration",
                "indexer_version": "1.0"
            },
            "download_statistics": self.download_stats,
            "datasets": {},
            "unified_statistics": {
                "total_files": 0,
                "total_size_bytes": 0,
                "available_datasets": 0,
                "character_coverage": 0,
                "estimated_training_samples": 0
            }
        }
        
        total_files = 0
        total_size = 0
        
        # æƒææ¯å€‹æ•¸æ“šé›†ç›®éŒ„
        for dataset_id, config in self.dataset_configs.items():
            dataset_dir = self.base_dir / dataset_id
            
            if dataset_dir.exists():
                dataset_info = {
                    "name": config["name"],
                    "description": config["description"],
                    "status": "downloaded",
                    "files": [],
                    "total_size": 0,
                    "file_count": 0
                }
                
                # æƒææ–‡ä»¶
                for file_path in dataset_dir.rglob("*"):
                    if file_path.is_file():
                        file_size = file_path.stat().st_size
                        file_info = {
                            "filename": file_path.name,
                            "path": str(file_path.relative_to(dataset_dir)),
                            "size": file_size,
                            "extension": file_path.suffix,
                            "modification_time": file_path.stat().st_mtime
                        }
                        dataset_info["files"].append(file_info)
                        dataset_info["total_size"] += file_size
                        total_size += file_size
                
                dataset_info["file_count"] = len(dataset_info["files"])
                total_files += dataset_info["file_count"]
                
                index_data["datasets"][dataset_id] = dataset_info
                index_data["unified_statistics"]["available_datasets"] += 1
                
                print(f"âœ… {config['name']}: {dataset_info['file_count']} æ–‡ä»¶, {dataset_info['total_size']/(1024*1024):.1f}MB")
            else:
                # æ•¸æ“šé›†æœªä¸‹è¼‰
                index_data["datasets"][dataset_id] = {
                    "name": config["name"],
                    "description": config["description"],
                    "status": "not_downloaded",
                    "download_urls": config["urls"],
                    "backup_info": config["backup_info"]
                }
                print(f"âš ï¸  {config['name']}: æœªä¸‹è¼‰")
        
        # æ›´æ–°çµ±è¨ˆ
        index_data["unified_statistics"]["total_files"] = total_files
        index_data["unified_statistics"]["total_size_bytes"] = total_size
        
        # ä¿å­˜ç´¢å¼•
        index_file = self.base_dir / "unified_dataset_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ çµ±ä¸€ç´¢å¼•å·²å‰µå»º: {index_file}")
        print(f"   ç¸½æ–‡ä»¶æ•¸: {total_files}")
        print(f"   ç¸½å¤§å°: {total_size/(1024*1024):.1f} MB")
        print(f"   å¯ç”¨æ•¸æ“šé›†: {index_data['unified_statistics']['available_datasets']}/{len(self.dataset_configs)}")
        
        return index_data
    
    def generate_integration_report(self, index_data: Dict):
        """ç”Ÿæˆæ•´åˆå ±å‘Š"""
        print("\nğŸ“Š === æ•¸æ“šé›†æ•´åˆå ±å‘Š ===")
        
        report = {
            "summary": {
                "execution_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_datasets_configured": len(self.dataset_configs),
                "successful_downloads": self.download_stats["successful_downloads"],
                "failed_downloads": self.download_stats["failed_downloads"],
                "download_success_rate": self.download_stats["successful_downloads"] / max(1, self.download_stats["attempted_downloads"]),
                "total_downloaded_mb": self.download_stats["total_size_downloaded"] / (1024*1024)
            },
            "dataset_status": {},
            "recommendations": [],
            "next_steps": []
        }
        
        # æ•¸æ“šé›†ç‹€æ…‹
        for dataset_id, dataset_info in index_data["datasets"].items():
            report["dataset_status"][dataset_id] = {
                "name": dataset_info["name"],
                "status": dataset_info["status"],
                "file_count": dataset_info.get("file_count", 0),
                "size_mb": dataset_info.get("total_size", 0) / (1024*1024)
            }
        
        # å»ºè­°
        if report["summary"]["successful_downloads"] < len(self.dataset_configs):
            report["recommendations"].append("è€ƒæ…®æ‰‹å‹•ä¸‹è¼‰æœªæˆåŠŸçš„æ•¸æ“šé›†")
        
        if report["summary"]["total_downloaded_mb"] < 100:
            report["recommendations"].append("ç•¶å‰æ•¸æ“šé‡è¼ƒå°ï¼Œå»ºè­°å¢åŠ æ›´å¤šæ•¸æ“šæº")
        
        report["recommendations"].extend([
            "å¯¦æ–½æ•¸æ“šé è™•ç†å’Œæ ¼å¼çµ±ä¸€",
            "å»ºç«‹æ•¸æ“šè³ªé‡æª¢æŸ¥æ©Ÿåˆ¶",
            "è¨­è¨ˆå¢é‡æ›´æ–°ç­–ç•¥"
        ])
        
        # ä¸‹ä¸€æ­¥
        report["next_steps"] = [
            "æ•¸æ“šé è™•ç†ï¼šçµ±ä¸€æ ¼å¼å’Œæ¨™è¨»",
            "è³ªé‡è©•ä¼°ï¼šæª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§",
            "æ•´åˆè¨“ç·´ï¼šèˆ‡OCR0712è¨“ç·´æµç¨‹å°æ¥",
            "æ€§èƒ½æ¸¬è©¦ï¼šå¤§è¦æ¨¡æ•¸æ“šè¨“ç·´æ•ˆæœè©•ä¼°",
            "å¢é‡æ›´æ–°ï¼šå®šæœŸç²å–æ–°æ•¸æ“š"
        ]
        
        # ä¿å­˜å ±å‘Š
        report_file = self.base_dir / "integration_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # é¡¯ç¤ºæ‘˜è¦
        print(f"ç¸½é…ç½®æ•¸æ“šé›†: {report['summary']['total_datasets_configured']}")
        print(f"æˆåŠŸä¸‹è¼‰: {report['summary']['successful_downloads']}")
        print(f"ä¸‹è¼‰å¤±æ•—: {report['summary']['failed_downloads']}")
        print(f"æˆåŠŸç‡: {report['summary']['download_success_rate']:.1%}")
        print(f"ä¸‹è¼‰ç¸½é‡: {report['summary']['total_downloaded_mb']:.1f} MB")
        print(f"\nğŸ“„ è©³ç´°å ±å‘Š: {report_file}")
        
        return report
    
    def run_large_dataset_integration(self):
        """é‹è¡Œå¤§å‹æ•¸æ“šé›†æ•´åˆæµç¨‹"""
        print("ğŸš€ é–‹å§‹OCR0712å¤§å‹æ•¸æ“šé›†æ•´åˆæµç¨‹...")
        print()
        
        try:
            # æ­¥é©Ÿ1: åˆ†æéœ€æ±‚
            requirements = self.analyze_dataset_requirements()
            
            # æ­¥é©Ÿ2: å‰µå»ºä¸‹è¼‰è¨ˆåŠƒ
            download_plan = self.create_dataset_download_plan()
            
            # æ­¥é©Ÿ3: ä¸‹è¼‰å¯ç²å–æ•¸æ“šé›†
            self.download_available_datasets(download_plan)
            
            # æ­¥é©Ÿ4: å‰µå»ºçµ±ä¸€ç´¢å¼•
            index_data = self.create_unified_dataset_index()
            
            # æ­¥é©Ÿ5: ç”Ÿæˆæ•´åˆå ±å‘Š
            report = self.generate_integration_report(index_data)
            
            print(f"\nğŸ‰ === å¤§å‹æ•¸æ“šé›†æ•´åˆå®Œæˆï¼ ===")
            print(f"ğŸ“ æ•¸æ“šç›®éŒ„: {self.base_dir.absolute()}")
            print(f"ğŸ“Š çµ±ä¸€ç´¢å¼•: unified_dataset_index.json")
            print(f"ğŸ“‹ æ•´åˆå ±å‘Š: integration_report.json")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•´åˆéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•¸"""
    integrator = LargeDatasetIntegrator()
    success = integrator.run_large_dataset_integration()
    
    if success:
        print("\nğŸ”„ === ä¸‹ä¸€æ­¥å»ºè­° ===")
        print("1. æ‰‹å‹•ç²å–éœ€è¦ç‰¹æ®Šè¨±å¯çš„æ•¸æ“šé›†")
        print("2. å¯¦æ–½æ•¸æ“šé è™•ç†å’Œæ ¼å¼çµ±ä¸€")
        print("3. èˆ‡OCR0712è¨“ç·´ç³»çµ±æ•´åˆ")
        print("4. åŸ·è¡Œå¤§è¦æ¨¡è¨“ç·´æ¸¬è©¦")
    else:
        print("\nğŸ’¡ æ•…éšœæ’é™¤å»ºè­°:")
        print("1. æª¢æŸ¥ç¶²çµ¡é€£æ¥")
        print("2. ç¢ºä¿æœ‰è¶³å¤ ç£ç›¤ç©ºé–“")
        print("3. æ‰‹å‹•ä¸‹è¼‰å¤±æ•—çš„æ•¸æ“šé›†")

if __name__ == "__main__":
    main()