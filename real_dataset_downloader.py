#!/usr/bin/env python3
"""
OCR0712 å¯¦éš›æ•¸æ“šé›†ä¸‹è¼‰å™¨ - å¯ç«‹å³åŸ·è¡Œ
æä¾›çœŸå¯¦å¯ç”¨çš„ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†ä¸‹è¼‰é€£çµ
"""

import os
import requests
import zipfile
import tarfile
from pathlib import Path
from tqdm import tqdm
import json
import struct
import numpy as np
from PIL import Image
import time

class RealDatasetDownloader:
    """çœŸå¯¦æ•¸æ“šé›†ä¸‹è¼‰å™¨"""
    
    def __init__(self, base_dir="./real_datasets"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        # å¯¦éš›å¯ç”¨çš„ä¸‹è¼‰URL
        self.dataset_urls = {
            # GitHubé–‹æºæ•¸æ“šé›† (ç«‹å³å¯ç”¨)
            "traditional_chinese_handwriting": {
                "url": "https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset/archive/refs/heads/main.zip",
                "size": "~45MB",
                "samples": "5,162 ç¹é«”ä¸­æ–‡æ‰‹å¯«åœ–åƒ",
                "format": "PNG + JSONæ¨™è¨»"
            },
            
            "thu_hcr_dataset": {
                "url": "https://github.com/thu-ml/thu-hcr/archive/refs/heads/main.zip", 
                "size": "~30MB",
                "samples": "æ¸…è¯å¤§å­¸æ‰‹å¯«è­˜åˆ¥æ•¸æ“šé›†",
                "format": "ç ”ç©¶ç´šæ¨™æº–æ ¼å¼"
            },
            
            # Kaggleæ•¸æ“šé›† (éœ€è¦API keyä½†å…è²»)
            "chinese_character_recognition": {
                "kaggle_dataset": "jeffreyhuang235/chinese-character-recognition",
                "size": "~120MB", 
                "samples": "ä¸­æ–‡å­—ç¬¦è­˜åˆ¥ç«¶è³½æ•¸æ“š",
                "format": "CSV + åœ–åƒ"
            },
            
            # CASIAå®˜æ–¹æ•¸æ“šé›† (éƒ¨åˆ†å…è²»)
            "casia_hwdb_sample": {
                "url": "http://www.nlpr.ia.ac.cn/databases/download/feature_data/HWDB1.0train_gnt.zip",
                "size": "~800MB",
                "samples": "CASIA-HWDB1.0è¨“ç·´é›†",
                "format": "GNTæ ¼å¼"
            },
            
            # å­¸è¡“æ©Ÿæ§‹æ•¸æ“šé›†
            "icdar_sample": {
                "url": "https://rrc.cvc.uab.es/downloads/ICDAR2013_Chinese_training.zip",
                "size": "~200MB",
                "samples": "ICDAR2013ä¸­æ–‡æ‰‹å¯«ç«¶è³½",
                "format": "æ¨™æº–ç«¶è³½æ ¼å¼"
            }
        }
    
    def download_with_progress(self, url, filename, timeout=30):
        """å¸¶é€²åº¦æ¢çš„æ–‡ä»¶ä¸‹è¼‰"""
        filepath = self.base_dir / filename
        
        if filepath.exists():
            print(f"âœ“ æ–‡ä»¶å·²å­˜åœ¨: {filename}")
            return str(filepath)
        
        try:
            print(f"ğŸ”„ é–‹å§‹ä¸‹è¼‰: {filename}")
            print(f"ğŸ“¡ URL: {url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(url, stream=True, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as f, tqdm(
                desc=filename,
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            print(f"âœ… ä¸‹è¼‰å®Œæˆ: {filename}")
            return str(filepath)
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä¸‹è¼‰å¤±æ•— {filename}: {e}")
            return None
        except Exception as e:
            print(f"âŒ æœªçŸ¥éŒ¯èª¤ {filename}: {e}")
            return None
    
    def extract_archive(self, filepath):
        """è§£å£“ç¸®æ–‡ä»¶"""
        print(f"ğŸ“¦ è§£å£“ç¸®: {Path(filepath).name}")
        
        try:
            if filepath.endswith('.zip'):
                with zipfile.ZipFile(filepath, 'r') as zip_ref:
                    zip_ref.extractall(self.base_dir)
            elif filepath.endswith(('.tar', '.tar.gz', '.tgz')):
                with tarfile.open(filepath, 'r:*') as tar_ref:
                    tar_ref.extractall(self.base_dir)
            
            print(f"âœ… è§£å£“å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ è§£å£“å¤±æ•—: {e}")
            return False
    
    def download_github_datasets(self):
        """ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›†ï¼ˆæœ€å¯é ï¼‰"""
        print("\nğŸ  === ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›† ===")
        
        github_datasets = [
            ("traditional_chinese_handwriting", "Traditional-Chinese-Handwriting.zip"),
            ("thu_hcr_dataset", "THU-HCR-Dataset.zip")
        ]
        
        success_count = 0
        
        for dataset_key, filename in github_datasets:
            dataset_info = self.dataset_urls[dataset_key]
            print(f"\nğŸ“‹ æ•¸æ“šé›†: {dataset_key}")
            print(f"ğŸ“Š å¤§å°: {dataset_info['size']}")
            print(f"ğŸ“ æ¨£æœ¬: {dataset_info['samples']}")
            
            filepath = self.download_with_progress(
                dataset_info["url"], 
                filename
            )
            
            if filepath:
                if self.extract_archive(filepath):
                    success_count += 1
                    self.validate_dataset(filepath.replace('.zip', ''))
        
        print(f"\nâœ… GitHubæ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ: {success_count}/{len(github_datasets)} æˆåŠŸ")
        return success_count > 0
    
    def download_casia_sample(self):
        """ä¸‹è¼‰CASIAæ¨£æœ¬æ•¸æ“š"""
        print("\nğŸ“ === ä¸‹è¼‰CASIAå­¸è¡“æ•¸æ“šé›† ===")
        
        dataset_info = self.dataset_urls["casia_hwdb_sample"]
        print(f"ğŸ“Š å¤§å°: {dataset_info['size']}")
        print(f"ğŸ“ æ¨£æœ¬: {dataset_info['samples']}")
        print("âš ï¸  æ³¨æ„: CASIAæ•¸æ“šé›†å¯èƒ½éœ€è¦å­¸è¡“æ©Ÿæ§‹éƒµç®±è¨»å†Š")
        
        filepath = self.download_with_progress(
            dataset_info["url"],
            "CASIA-HWDB1.0-sample.zip"
        )
        
        if filepath:
            if self.extract_archive(filepath):
                # è™•ç†GNTæ ¼å¼
                self.process_gnt_format(self.base_dir)
                return True
        
        return False
    
    def setup_kaggle_api(self):
        """è¨­ç½®Kaggle API"""
        try:
            import kaggle
            print("âœ… Kaggle APIå·²å¯ç”¨")
            return True
        except ImportError:
            print("âŒ éœ€è¦å®‰è£Kaggle API: pip install kaggle")
            return False
        except Exception as e:
            print(f"âš ï¸  Kaggle APIè¨­ç½®å•é¡Œ: {e}")
            print("è«‹ç¢ºä¿å·²è¨­ç½® ~/.kaggle/kaggle.json")
            return False
    
    def download_kaggle_datasets(self):
        """ä¸‹è¼‰Kaggleæ•¸æ“šé›†"""
        print("\nğŸ† === ä¸‹è¼‰Kaggleç«¶è³½æ•¸æ“šé›† ===")
        
        if not self.setup_kaggle_api():
            return False
        
        try:
            import kaggle
            
            dataset_name = self.dataset_urls["chinese_character_recognition"]["kaggle_dataset"]
            output_path = str(self.base_dir / "kaggle_chinese_handwriting")
            
            print(f"ğŸ“¡ ä¸‹è¼‰æ•¸æ“šé›†: {dataset_name}")
            kaggle.api.dataset_download_files(
                dataset_name, 
                path=output_path, 
                unzip=True
            )
            
            print("âœ… Kaggleæ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ Kaggleä¸‹è¼‰å¤±æ•—: {e}")
            return False
    
    def process_gnt_format(self, base_path):
        """è™•ç†CASIA GNTæ ¼å¼æ•¸æ“š"""
        print("ğŸ”„ è™•ç†GNTæ ¼å¼æ•¸æ“š...")
        
        # æŸ¥æ‰¾GNTæ–‡ä»¶
        gnt_files = list(Path(base_path).glob("**/*.gnt"))
        
        if not gnt_files:
            print("âš ï¸  æœªæ‰¾åˆ°GNTæ–‡ä»¶")
            return
        
        output_dir = base_path / "processed_casia"
        output_dir.mkdir(exist_ok=True)
        
        for gnt_file in gnt_files[:1]:  # è™•ç†ç¬¬ä¸€å€‹æ–‡ä»¶ä½œç‚ºç¤ºä¾‹
            print(f"ğŸ“– è™•ç†æ–‡ä»¶: {gnt_file.name}")
            
            try:
                samples = self.read_gnt_file(gnt_file)
                
                for i, sample in enumerate(samples[:100]):  # è™•ç†å‰100å€‹æ¨£æœ¬
                    # ä¿å­˜åœ–åƒ
                    image = Image.fromarray(sample['image'])
                    image_path = output_dir / f"sample_{i:04d}.png"
                    image.save(image_path)
                    
                    # ä¿å­˜æ¨™è¨»
                    annotation = {
                        'text': sample['char'],
                        'label': int(sample['label']),
                        'source': 'CASIA-HWDB',
                        'image_file': image_path.name
                    }
                    
                    json_path = output_dir / f"sample_{i:04d}.json"
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(annotation, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… è™•ç†å®Œæˆ: {len(samples[:100])} å€‹æ¨£æœ¬")
                
            except Exception as e:
                print(f"âŒ è™•ç†GNTæ–‡ä»¶å¤±æ•—: {e}")
    
    def read_gnt_file(self, gnt_path):
        """è®€å–GNTæ ¼å¼æ–‡ä»¶"""
        samples = []
        
        try:
            with open(gnt_path, 'rb') as f:
                while True:
                    try:
                        # è®€å–æ¨£æœ¬å¤§å°
                        sample_size_bytes = f.read(4)
                        if len(sample_size_bytes) < 4:
                            break
                        
                        sample_size = struct.unpack('<I', sample_size_bytes)[0]
                        
                        # è®€å–æ¨™ç±¤
                        label = struct.unpack('<H', f.read(2))[0]
                        
                        # è®€å–åœ–åƒå°ºå¯¸
                        width = struct.unpack('<H', f.read(2))[0]
                        height = struct.unpack('<H', f.read(2))[0]
                        
                        # è®€å–åœ–åƒæ•¸æ“š
                        image_size = width * height
                        image_data = f.read(image_size)
                        
                        if len(image_data) < image_size:
                            break
                        
                        # è½‰æ›ç‚ºåœ–åƒ
                        image = np.frombuffer(image_data, dtype=np.uint8)
                        image = image.reshape(height, width)
                        
                        # è½‰æ›ç‚ºå­—ç¬¦
                        try:
                            char = chr(label) if 0 < label < 65536 else f"\\u{label:04x}"
                        except ValueError:
                            char = "?"
                        
                        samples.append({
                            'image': image,
                            'label': label,
                            'char': char,
                            'width': width,
                            'height': height
                        })
                        
                    except struct.error:
                        break
                    except Exception as e:
                        print(f"âš ï¸  æ¨£æœ¬è§£æéŒ¯èª¤: {e}")
                        break
                        
        except Exception as e:
            print(f"âŒ æ–‡ä»¶è®€å–éŒ¯èª¤: {e}")
        
        return samples
    
    def validate_dataset(self, dataset_path):
        """é©—è­‰æ•¸æ“šé›†"""
        print(f"ğŸ” é©—è­‰æ•¸æ“šé›†: {dataset_path}")
        
        if isinstance(dataset_path, str):
            dataset_path = Path(dataset_path)
        
        # çµ±è¨ˆåœ–åƒæ–‡ä»¶
        image_files = list(dataset_path.glob("**/*.png")) + list(dataset_path.glob("**/*.jpg"))
        json_files = list(dataset_path.glob("**/*.json"))
        
        print(f"ğŸ“Š åœ–åƒæ–‡ä»¶: {len(image_files)}")
        print(f"ğŸ“ æ¨™è¨»æ–‡ä»¶: {len(json_files)}")
        
        if image_files:
            # æª¢æŸ¥å¹¾å€‹æ¨£æœ¬
            sample_count = min(5, len(image_files))
            print(f"ğŸ” æª¢æŸ¥ {sample_count} å€‹æ¨£æœ¬...")
            
            for i, image_file in enumerate(image_files[:sample_count]):
                try:
                    with Image.open(image_file) as img:
                        print(f"  âœ… {image_file.name}: {img.size} {img.mode}")
                except Exception as e:
                    print(f"  âŒ {image_file.name}: {e}")
        
        return len(image_files)
    
    def create_unified_dataset(self):
        """å‰µå»ºçµ±ä¸€æ ¼å¼çš„æ•¸æ“šé›†"""
        print("\nğŸ”§ === å‰µå»ºçµ±ä¸€æ•¸æ“šé›†æ ¼å¼ ===")
        
        unified_dir = self.base_dir / "unified_dataset"
        unified_dir.mkdir(exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰æ•¸æ“šé›†
        all_datasets = []
        
        # æƒææ‰€æœ‰å­ç›®éŒ„
        for subdir in self.base_dir.iterdir():
            if subdir.is_dir() and subdir.name != "unified_dataset":
                dataset_info = self.scan_dataset_directory(subdir)
                if dataset_info['sample_count'] > 0:
                    all_datasets.append(dataset_info)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_datasets)} å€‹æ•¸æ“šé›†")
        
        # åˆä½µæ•¸æ“šé›†
        total_samples = 0
        for i, dataset in enumerate(all_datasets):
            print(f"ğŸ”„ è™•ç†æ•¸æ“šé›† {i+1}/{len(all_datasets)}: {dataset['name']}")
            
            samples_copied = self.copy_dataset_samples(
                dataset, 
                unified_dir, 
                prefix=f"ds{i:02d}"
            )
            
            total_samples += samples_copied
            print(f"  âœ… è¤‡è£½äº† {samples_copied} å€‹æ¨£æœ¬")
        
        print(f"ğŸ‰ çµ±ä¸€æ•¸æ“šé›†å‰µå»ºå®Œæˆ: {total_samples} å€‹æ¨£æœ¬")
        
        # å‰µå»ºæ•¸æ“šé›†çµ±è¨ˆ
        self.create_dataset_statistics(unified_dir, all_datasets)
    
    def scan_dataset_directory(self, directory):
        """æƒææ•¸æ“šé›†ç›®éŒ„"""
        image_files = list(directory.glob("**/*.png")) + list(directory.glob("**/*.jpg"))
        json_files = list(directory.glob("**/*.json"))
        
        return {
            'name': directory.name,
            'path': directory,
            'sample_count': len(image_files),
            'annotation_count': len(json_files),
            'image_files': image_files,
            'json_files': json_files
        }
    
    def copy_dataset_samples(self, dataset_info, target_dir, prefix="sample"):
        """è¤‡è£½æ•¸æ“šé›†æ¨£æœ¬åˆ°çµ±ä¸€ç›®éŒ„"""
        samples_copied = 0
        
        for i, image_file in enumerate(dataset_info['image_files'][:1000]):  # æœ€å¤š1000å€‹æ¨£æœ¬
            try:
                # è¤‡è£½åœ–åƒ
                target_image = target_dir / f"{prefix}_{i:06d}.png"
                
                with Image.open(image_file) as img:
                    # æ¨™æº–åŒ–ç‚º224x224
                    img_resized = img.resize((224, 224))
                    if img_resized.mode != 'RGB':
                        img_resized = img_resized.convert('RGB')
                    img_resized.save(target_image)
                
                # å‰µå»ºæˆ–è¤‡è£½æ¨™è¨»
                json_file = image_file.with_suffix('.json')
                target_json = target_dir / f"{prefix}_{i:06d}.json"
                
                if json_file.exists():
                    # è¤‡è£½ç¾æœ‰æ¨™è¨»
                    with open(json_file, 'r', encoding='utf-8') as f:
                        annotation = json.load(f)
                else:
                    # å‰µå»ºåŸºç¤æ¨™è¨»
                    annotation = {
                        'text': f"sample_{i}",
                        'source_dataset': dataset_info['name'],
                        'original_file': image_file.name
                    }
                
                annotation['unified_id'] = f"{prefix}_{i:06d}"
                
                with open(target_json, 'w', encoding='utf-8') as f:
                    json.dump(annotation, f, ensure_ascii=False, indent=2)
                
                samples_copied += 1
                
            except Exception as e:
                print(f"  âš ï¸  æ¨£æœ¬è¤‡è£½å¤±æ•— {image_file.name}: {e}")
        
        return samples_copied
    
    def create_dataset_statistics(self, unified_dir, datasets):
        """å‰µå»ºæ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'creation_time': time.time(),
            'total_samples': len(list(unified_dir.glob("*.png"))),
            'source_datasets': [
                {
                    'name': ds['name'],
                    'original_samples': ds['sample_count'],
                    'included_samples': min(1000, ds['sample_count'])
                }
                for ds in datasets
            ],
            'format': {
                'images': 'PNG 224x224 RGB',
                'annotations': 'JSON with text and metadata'
            }
        }
        
        stats_file = unified_dir / "dataset_info.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š æ•¸æ“šé›†çµ±è¨ˆå·²ä¿å­˜: {stats_file}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ === OCR0712 çœŸå¯¦æ•¸æ“šé›†ä¸‹è¼‰å™¨ ===")
    print()
    
    downloader = RealDatasetDownloader()
    
    print("è«‹é¸æ“‡ä¸‹è¼‰é¸é …:")
    print("1. ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›† (æ¨è–¦ï¼Œæœ€å¯é )")
    print("2. ä¸‹è¼‰CASIAå­¸è¡“æ•¸æ“šé›† (å¯èƒ½éœ€è¦è¨»å†Š)")
    print("3. ä¸‹è¼‰Kaggleç«¶è³½æ•¸æ“šé›† (éœ€è¦API key)")
    print("4. ä¸‹è¼‰å…¨éƒ¨å¯ç”¨æ•¸æ“šé›†")
    print("5. åƒ…å‰µå»ºçµ±ä¸€æ•¸æ“šé›†æ ¼å¼")
    
    choice = input("\nè«‹é¸æ“‡ (1-5): ").strip()
    
    success = False
    
    if choice == "1":
        success = downloader.download_github_datasets()
    elif choice == "2":
        success = downloader.download_casia_sample()
    elif choice == "3":
        success = downloader.download_kaggle_datasets()
    elif choice == "4":
        print("ğŸ”„ é–‹å§‹ä¸‹è¼‰å…¨éƒ¨æ•¸æ“šé›†...")
        github_success = downloader.download_github_datasets()
        casia_success = downloader.download_casia_sample()
        kaggle_success = downloader.download_kaggle_datasets()
        success = any([github_success, casia_success, kaggle_success])
    elif choice == "5":
        downloader.create_unified_dataset()
        success = True
    else:
        print("âŒ ç„¡æ•ˆé¸æ“‡")
        return
    
    if success and choice != "5":
        print("\nğŸ”§ å‰µå»ºçµ±ä¸€æ•¸æ“šé›†...")
        downloader.create_unified_dataset()
    
    if success:
        print("\nğŸ‰ === ä¸‹è¼‰å®Œæˆ ===")
        print(f"ğŸ“ æ•¸æ“šé›†ä½ç½®: {downloader.base_dir.absolute()}")
        print("ğŸ’¡ æ¥ä¸‹ä¾†å¯ä»¥:")
        print("   1. æª¢æŸ¥ unified_dataset/ ç›®éŒ„")
        print("   2. é‹è¡Œ local_training_system.py é–‹å§‹è¨“ç·´")
        print("   3. ä½¿ç”¨ software_rl_gym.py é€²è¡ŒRLå„ªåŒ–")
    else:
        print("\nâŒ ä¸‹è¼‰æœªå®Œæˆï¼Œè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥æˆ–é¸æ“‡å…¶ä»–æ•¸æ“šæº")

if __name__ == "__main__":
    main()