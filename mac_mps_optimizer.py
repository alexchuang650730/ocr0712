#!/usr/bin/env python3
"""
OCR0712 Mac MPSç¡¬ä»¶åŠ é€Ÿå„ªåŒ–ç³»çµ±
é‡å°Apple Silicon Macé€²è¡ŒGPUåŠ é€Ÿå’Œè¨“ç·´æ•ˆç‡å„ªåŒ–
"""

import os
import sys
import json
import time
import numpy as np
import platform
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import subprocess
import psutil

@dataclass
class MPSConfig:
    """MPSé…ç½®"""
    # MPSè¨­ç½®
    enable_mps: bool = True
    memory_fraction: float = 0.8
    allow_tf32: bool = True
    
    # æ‰¹æ¬¡å„ªåŒ–
    batch_size: int = 64
    gradient_accumulation_steps: int = 4
    max_sequence_length: int = 512
    
    # å…§å­˜ç®¡ç†
    memory_optimization: bool = True
    gradient_checkpointing: bool = True
    mixed_precision: bool = True
    
    # ä¸¦è¡ŒåŒ–
    num_workers: int = 4
    pin_memory: bool = True
    persistent_workers: bool = True
    
    # æ€§èƒ½èª¿å„ª
    compile_model: bool = True
    cache_dataset: bool = True
    prefetch_factor: int = 2
    
    # Macç‰¹å®šå„ªåŒ–
    use_metal_performance_shaders: bool = True
    optimize_for_m1_m2: bool = True
    energy_efficient_mode: bool = False

class SystemProfiler:
    """ç³»çµ±æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.system_info = self._collect_system_info()
        self.performance_baseline = {}
        
        print(f"ğŸ’» === Macç³»çµ±æ€§èƒ½åˆ†æå™¨ ===")
        print(f"ğŸ–¥ï¸  ç³»çµ±: {self.system_info['system']}")
        print(f"ğŸ”‹ è™•ç†å™¨: {self.system_info['processor']}")
        print(f"ğŸ’¾ è¨˜æ†¶é«”: {self.system_info['memory_gb']:.1f} GB")
        print()
    
    def _collect_system_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»çµ±ä¿¡æ¯"""
        info = {
            "system": platform.system(),
            "platform": platform.platform(),
            "processor": platform.processor(),
            "architecture": platform.architecture()[0],
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(logical=True),
            "cpu_count_physical": psutil.cpu_count(logical=False),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "disk_usage": psutil.disk_usage('/').percent
        }
        
        # Macç‰¹å®šä¿¡æ¯
        if platform.system() == "Darwin":
            try:
                # ç²å–Macå‹è™Ÿ
                result = subprocess.run(['system_profiler', 'SPHardwareDataType'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    lines = result.stdout.split('\n')
                    for line in lines:
                        if 'Model Name' in line:
                            info['mac_model'] = line.split(':')[1].strip()
                        elif 'Chip' in line and ('Apple' in line or 'M1' in line or 'M2' in line):
                            info['chip'] = line.split(':')[1].strip()
                        elif 'Total Number of Cores' in line:
                            info['total_cores'] = line.split(':')[1].strip()
                
                # æª¢æŸ¥MPSå¯ç”¨æ€§
                info['mps_available'] = self._check_mps_availability()
                
            except Exception as e:
                print(f"âš ï¸  ç²å–Macä¿¡æ¯å¤±æ•—: {e}")
                info['mac_model'] = "Unknown Mac"
                info['mps_available'] = False
        else:
            info['mps_available'] = False
        
        return info
    
    def _check_mps_availability(self) -> bool:
        """æª¢æŸ¥MPSå¯ç”¨æ€§"""
        try:
            # å˜—è©¦å°å…¥PyTorchä¸¦æª¢æŸ¥MPS
            import torch
            return torch.backends.mps.is_available() and torch.backends.mps.is_built()
        except ImportError:
            return False
        except Exception:
            return False
    
    def benchmark_cpu_performance(self) -> Dict[str, float]:
        """åŸºæº–æ¸¬è©¦CPUæ€§èƒ½"""
        print("ğŸ”„ åŸ·è¡ŒCPUæ€§èƒ½åŸºæº–æ¸¬è©¦...")
        
        # çŸ©é™£é‹ç®—æ¸¬è©¦
        start_time = time.time()
        
        # æ¸¬è©¦1: å¤§çŸ©é™£ä¹˜æ³•
        matrix_size = 1000
        a = np.random.randn(matrix_size, matrix_size).astype(np.float32)
        b = np.random.randn(matrix_size, matrix_size).astype(np.float32)
        
        matrix_start = time.time()
        c = np.dot(a, b)
        matrix_time = time.time() - matrix_start
        
        # æ¸¬è©¦2: FFTè®Šæ›
        fft_start = time.time()
        data = np.random.randn(1024*1024).astype(np.float32)
        fft_result = np.fft.fft(data)
        fft_time = time.time() - fft_start
        
        # æ¸¬è©¦3: å¤šç·šç¨‹è¨ˆç®—
        multithread_start = time.time()
        results = []
        for _ in range(4):
            result = np.sum(np.random.randn(1000, 1000) ** 2)
            results.append(result)
        multithread_time = time.time() - multithread_start
        
        total_time = time.time() - start_time
        
        benchmark_results = {
            "matrix_multiplication_time": matrix_time,
            "fft_time": fft_time,
            "multithread_time": multithread_time,
            "total_benchmark_time": total_time,
            "matrix_gflops": (2 * matrix_size**3) / (matrix_time * 1e9),
            "overall_score": 100 / total_time  # ç°¡åŒ–è©•åˆ†
        }
        
        print(f"   çŸ©é™£ä¹˜æ³•: {matrix_time:.3f}s ({benchmark_results['matrix_gflops']:.1f} GFLOPS)")
        print(f"   FFTè®Šæ›: {fft_time:.3f}s")
        print(f"   å¤šç·šç¨‹: {multithread_time:.3f}s")
        print(f"   æ•´é«”è©•åˆ†: {benchmark_results['overall_score']:.1f}")
        
        return benchmark_results
    
    def benchmark_memory_performance(self) -> Dict[str, float]:
        """åŸºæº–æ¸¬è©¦å…§å­˜æ€§èƒ½"""
        print("ğŸ”„ åŸ·è¡Œå…§å­˜æ€§èƒ½åŸºæº–æ¸¬è©¦...")
        
        # å…§å­˜å¸¶å¯¬æ¸¬è©¦
        data_size = 100 * 1024 * 1024  # 100MB
        
        # æ¸¬è©¦1: é †åºè®€å¯«
        start_time = time.time()
        data = np.random.randn(data_size // 4).astype(np.float32)
        sequential_write_time = time.time() - start_time
        
        start_time = time.time()
        total = np.sum(data)
        sequential_read_time = time.time() - start_time
        
        # æ¸¬è©¦2: éš¨æ©Ÿè¨ªå•
        indices = np.random.randint(0, len(data), size=len(data)//10)
        start_time = time.time()
        random_data = data[indices]
        random_access_time = time.time() - start_time
        
        # æ¸¬è©¦3: å…§å­˜æ‹·è²
        start_time = time.time()
        data_copy = data.copy()
        memory_copy_time = time.time() - start_time
        
        # è¨ˆç®—å¸¶å¯¬ (MB/s)
        data_size_mb = data_size / (1024 * 1024)
        
        memory_results = {
            "sequential_write_time": sequential_write_time,
            "sequential_read_time": sequential_read_time,
            "random_access_time": random_access_time,
            "memory_copy_time": memory_copy_time,
            "write_bandwidth_mb_s": data_size_mb / sequential_write_time,
            "read_bandwidth_mb_s": data_size_mb / sequential_read_time,
            "copy_bandwidth_mb_s": data_size_mb / memory_copy_time
        }
        
        print(f"   å¯«å…¥å¸¶å¯¬: {memory_results['write_bandwidth_mb_s']:.1f} MB/s")
        print(f"   è®€å–å¸¶å¯¬: {memory_results['read_bandwidth_mb_s']:.1f} MB/s")
        print(f"   æ‹·è²å¸¶å¯¬: {memory_results['copy_bandwidth_mb_s']:.1f} MB/s")
        
        return memory_results

class MPSOptimizer:
    """MPSå„ªåŒ–å™¨"""
    
    def __init__(self, config: MPSConfig, system_info: Dict[str, Any]):
        self.config = config
        self.system_info = system_info
        self.optimization_applied = []
        self.performance_metrics = {}
        
        print(f"âš¡ === MPSç¡¬ä»¶åŠ é€Ÿå„ªåŒ–å™¨ ===")
        print(f"ğŸ”§ MPSå¯ç”¨: {system_info.get('mps_available', False)}")
        print(f"ğŸ’¾ å…§å­˜å„ªåŒ–: {config.memory_optimization}")
        print(f"ğŸš€ æ¨¡å‹ç·¨è­¯: {config.compile_model}")
        print()
    
    def setup_mps_environment(self) -> bool:
        """è¨­ç½®MPSç’°å¢ƒ"""
        if not self.config.enable_mps:
            print("âŒ MPSå·²ç¦ç”¨")
            return False
        
        if not self.system_info.get('mps_available', False):
            print("âŒ MPSä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            return False
        
        try:
            # è¨­ç½®ç’°å¢ƒè®Šé‡
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = str(self.config.memory_fraction)
            
            # å¦‚æœæ˜¯M1/M2 Macï¼Œå•Ÿç”¨ç‰¹å®šå„ªåŒ–
            if self.system_info.get('chip', '').startswith('Apple'):
                os.environ['METAL_DEVICE_WRAPPER_TYPE'] = '1'
                os.environ['MTL_HUD_ENABLED'] = '0'  # ç¦ç”¨Metal HUDä»¥æé«˜æ€§èƒ½
                self.optimization_applied.append("apple_silicon_optimization")
            
            print("âœ… MPSç’°å¢ƒè¨­ç½®å®Œæˆ")
            self.optimization_applied.append("mps_environment")
            return True
            
        except Exception as e:
            print(f"âŒ MPSç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
            return False
    
    def optimize_memory_usage(self) -> Dict[str, Any]:
        """å„ªåŒ–å…§å­˜ä½¿ç”¨"""
        print("ğŸ”„ å„ªåŒ–å…§å­˜ä½¿ç”¨...")
        
        optimizations = {}
        
        # 1. è‡ªå‹•åƒåœ¾å›æ”¶
        try:
            import gc
            gc.collect()
            optimizations["garbage_collection"] = "enabled"
        except:
            pass
        
        # 2. è¨­ç½®æœ€ä½³æ‰¹æ¬¡å¤§å°
        available_memory = self.system_info['memory_gb']
        
        # åŸºæ–¼å¯ç”¨å…§å­˜èª¿æ•´æ‰¹æ¬¡å¤§å°
        if available_memory >= 32:
            optimal_batch_size = 128
        elif available_memory >= 16:
            optimal_batch_size = 64
        elif available_memory >= 8:
            optimal_batch_size = 32
        else:
            optimal_batch_size = 16
        
        # è€ƒæ…®MPSå…§å­˜é™åˆ¶
        if self.config.enable_mps and optimal_batch_size > 64:
            optimal_batch_size = 64  # MPSå»ºè­°çš„æœ€å¤§æ‰¹æ¬¡å¤§å°
        
        self.config.batch_size = min(self.config.batch_size, optimal_batch_size)
        optimizations["optimal_batch_size"] = self.config.batch_size
        
        # 3. æ¢¯åº¦ç´¯ç©å„ªåŒ–
        if available_memory < 16:
            self.config.gradient_accumulation_steps = max(4, self.config.gradient_accumulation_steps)
            optimizations["gradient_accumulation"] = self.config.gradient_accumulation_steps
        
        # 4. å·¥ä½œé€²ç¨‹å„ªåŒ–
        cpu_count = self.system_info['cpu_count']
        if cpu_count >= 8:
            optimal_workers = 6
        elif cpu_count >= 4:
            optimal_workers = 4
        else:
            optimal_workers = 2
        
        self.config.num_workers = min(self.config.num_workers, optimal_workers)
        optimizations["num_workers"] = self.config.num_workers
        
        print(f"   æ‰¹æ¬¡å¤§å°: {optimizations['optimal_batch_size']}")
        print(f"   å·¥ä½œé€²ç¨‹: {optimizations['num_workers']}")
        print(f"   æ¢¯åº¦ç´¯ç©: {optimizations.get('gradient_accumulation', 'unchanged')}")
        
        self.optimization_applied.append("memory_optimization")
        return optimizations
    
    def setup_data_loading_optimization(self) -> Dict[str, Any]:
        """è¨­ç½®æ•¸æ“šåŠ è¼‰å„ªåŒ–"""
        print("ğŸ”„ è¨­ç½®æ•¸æ“šåŠ è¼‰å„ªåŒ–...")
        
        optimizations = {
            "pin_memory": self.config.pin_memory,
            "num_workers": self.config.num_workers,
            "persistent_workers": self.config.persistent_workers,
            "prefetch_factor": self.config.prefetch_factor
        }
        
        # Macç‰¹å®šå„ªåŒ–
        if platform.system() == "Darwin":
            # Macä¸Šå»ºè­°è¼ƒå°‘çš„workeræ•¸é‡
            optimizations["num_workers"] = min(4, optimizations["num_workers"])
            
            # å•Ÿç”¨é å–
            optimizations["prefetch_factor"] = max(2, optimizations["prefetch_factor"])
        
        print(f"   Pin Memory: {optimizations['pin_memory']}")
        print(f"   Workers: {optimizations['num_workers']}")
        print(f"   Prefetch Factor: {optimizations['prefetch_factor']}")
        
        self.optimization_applied.append("data_loading_optimization")
        return optimizations
    
    def create_optimized_training_config(self) -> Dict[str, Any]:
        """å‰µå»ºå„ªåŒ–çš„è¨“ç·´é…ç½®"""
        print("ğŸ”„ å‰µå»ºå„ªåŒ–è¨“ç·´é…ç½®...")
        
        training_config = {
            # åŸºç¤è¨­ç½®
            "device": "mps" if (self.config.enable_mps and self.system_info.get('mps_available')) else "cpu",
            "batch_size": self.config.batch_size,
            "gradient_accumulation_steps": self.config.gradient_accumulation_steps,
            
            # å…§å­˜å„ªåŒ–
            "memory_optimization": self.config.memory_optimization,
            "gradient_checkpointing": self.config.gradient_checkpointing,
            "mixed_precision": self.config.mixed_precision and self.config.enable_mps,
            
            # ä¸¦è¡ŒåŒ–è¨­ç½®
            "num_workers": self.config.num_workers,
            "pin_memory": self.config.pin_memory,
            "persistent_workers": self.config.persistent_workers,
            
            # æ€§èƒ½å„ªåŒ–
            "compile_model": self.config.compile_model,
            "cache_dataset": self.config.cache_dataset,
            
            # Macç‰¹å®šå„ªåŒ–
            "use_metal_performance_shaders": self.config.use_metal_performance_shaders,
            "optimize_for_apple_silicon": self.system_info.get('chip', '').startswith('Apple'),
            "energy_efficient_mode": self.config.energy_efficient_mode
        }
        
        # æ ¹æ“šç³»çµ±èª¿æ•´é…ç½®
        if self.system_info['memory_gb'] < 8:
            training_config["gradient_checkpointing"] = True
            training_config["mixed_precision"] = True
        
        print(f"   è¨­å‚™: {training_config['device']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {training_config['batch_size']}")
        print(f"   æ··åˆç²¾åº¦: {training_config['mixed_precision']}")
        print(f"   æ¢¯åº¦æª¢æŸ¥é»: {training_config['gradient_checkpointing']}")
        
        self.optimization_applied.append("training_config_optimization")
        return training_config
    
    def benchmark_training_performance(self, config: Dict[str, Any]) -> Dict[str, float]:
        """åŸºæº–æ¸¬è©¦è¨“ç·´æ€§èƒ½"""
        print("ğŸ”„ åŸ·è¡Œè¨“ç·´æ€§èƒ½åŸºæº–æ¸¬è©¦...")
        
        # æ¨¡æ“¬è¨“ç·´å·¥ä½œè² è¼‰
        batch_size = config['batch_size']
        sequence_length = 512
        hidden_dim = 256
        
        # æ¨¡æ“¬å‰å‘å‚³æ’­
        start_time = time.time()
        
        # å‰µå»ºæ¨¡æ“¬æ•¸æ“š
        input_data = np.random.randn(batch_size, sequence_length, hidden_dim).astype(np.float32)
        weights = np.random.randn(hidden_dim, hidden_dim).astype(np.float32)
        
        # æ¨¡æ“¬å¤šå±¤è¨ˆç®—
        output = input_data
        for layer in range(6):  # 6å±¤ç¶²çµ¡
            output = np.tanh(np.dot(output, weights))
        
        forward_time = time.time() - start_time
        
        # æ¨¡æ“¬åå‘å‚³æ’­
        start_time = time.time()
        
        # ç°¡åŒ–çš„æ¢¯åº¦è¨ˆç®—
        gradients = []
        for layer in range(6):
            grad = np.random.randn(*weights.shape).astype(np.float32)
            gradients.append(grad)
        
        backward_time = time.time() - start_time
        
        # è¨ˆç®—ååé‡
        total_time = forward_time + backward_time
        samples_per_second = batch_size / total_time
        tokens_per_second = (batch_size * sequence_length) / total_time
        
        performance_metrics = {
            "forward_time": forward_time,
            "backward_time": backward_time,
            "total_time": total_time,
            "samples_per_second": samples_per_second,
            "tokens_per_second": tokens_per_second,
            "memory_efficiency": 1.0 / (batch_size * sequence_length / 1000),  # ç°¡åŒ–æŒ‡æ¨™
            "optimization_score": samples_per_second * 10  # ç°¡åŒ–è©•åˆ†
        }
        
        print(f"   å‰å‘å‚³æ’­: {forward_time:.3f}s")
        print(f"   åå‘å‚³æ’­: {backward_time:.3f}s") 
        print(f"   æ¨£æœ¬/ç§’: {samples_per_second:.1f}")
        print(f"   Token/ç§’: {tokens_per_second:.0f}")
        
        self.performance_metrics = performance_metrics
        return performance_metrics
    
    def generate_optimization_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report = {
            "system_information": self.system_info,
            "mps_configuration": {
                "enable_mps": self.config.enable_mps,
                "memory_fraction": self.config.memory_fraction,
                "mixed_precision": self.config.mixed_precision,
                "compile_model": self.config.compile_model
            },
            "optimizations_applied": self.optimization_applied,
            "performance_metrics": self.performance_metrics,
            "recommendations": self._generate_recommendations(),
            "next_steps": [
                "å¯¦æ–½å¯¦éš›æ¨¡å‹è¨“ç·´æ¸¬è©¦",
                "ç›£æ§è¨“ç·´éç¨‹ä¸­çš„å…§å­˜ä½¿ç”¨",
                "èª¿æ•´æ‰¹æ¬¡å¤§å°ä»¥é”åˆ°æœ€ä½³æ€§èƒ½",
                "æ¸¬è©¦ä¸åŒæ¨¡å‹æ¶æ§‹çš„æ€§èƒ½è¡¨ç¾"
            ]
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ç³»çµ±é…ç½®çš„å»ºè­°
        if self.system_info['memory_gb'] < 16:
            recommendations.append("å…§å­˜è¼ƒå°‘ï¼Œå»ºè­°å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»å’Œæ··åˆç²¾åº¦è¨“ç·´")
        
        if not self.system_info.get('mps_available', False):
            recommendations.append("MPSä¸å¯ç”¨ï¼Œè€ƒæ…®å‡ç´šåˆ°æ”¯æŒMPSçš„Macæˆ–ä½¿ç”¨CPUå„ªåŒ–")
        
        if self.system_info['cpu_count'] >= 8:
            recommendations.append("å¤šæ ¸CPUï¼Œå¯ä»¥è€ƒæ…®å¢åŠ æ•¸æ“šåŠ è¼‰å™¨çš„å·¥ä½œé€²ç¨‹æ•¸")
        
        # åŸºæ–¼æ€§èƒ½æŒ‡æ¨™çš„å»ºè­°
        if self.performance_metrics:
            if self.performance_metrics.get('samples_per_second', 0) < 10:
                recommendations.append("è¨“ç·´é€Ÿåº¦è¼ƒæ…¢ï¼Œè€ƒæ…®æ¸›å°‘æ‰¹æ¬¡å¤§å°æˆ–å•Ÿç”¨æ›´å¤šå„ªåŒ–")
            
            if self.performance_metrics.get('memory_efficiency', 0) < 0.5:
                recommendations.append("å…§å­˜æ•ˆç‡è¼ƒä½ï¼Œå»ºè­°èª¿æ•´åºåˆ—é•·åº¦æˆ–æ‰¹æ¬¡å¤§å°")
        
        if not recommendations:
            recommendations.append("ç³»çµ±é…ç½®è‰¯å¥½ï¼Œç¹¼çºŒç•¶å‰å„ªåŒ–è¨­ç½®")
        
        return recommendations

def main():
    """ä¸»å‡½æ•¸"""
    print("âš¡ === OCR0712 Mac MPSç¡¬ä»¶åŠ é€Ÿå„ªåŒ– ===")
    print()
    
    # 1. ç³»çµ±æ€§èƒ½åˆ†æ
    profiler = SystemProfiler()
    
    # CPUæ€§èƒ½åŸºæº–æ¸¬è©¦
    cpu_benchmark = profiler.benchmark_cpu_performance()
    
    # å…§å­˜æ€§èƒ½åŸºæº–æ¸¬è©¦  
    memory_benchmark = profiler.benchmark_memory_performance()
    
    # 2. å‰µå»ºMPSé…ç½®
    mps_config = MPSConfig(
        enable_mps=True,
        memory_optimization=True,
        compile_model=True,
        batch_size=64
    )
    
    # 3. å‰µå»ºMPSå„ªåŒ–å™¨
    optimizer = MPSOptimizer(mps_config, profiler.system_info)
    
    # 4. è¨­ç½®MPSç’°å¢ƒ
    mps_ready = optimizer.setup_mps_environment()
    
    # 5. å…§å­˜å„ªåŒ–
    memory_optimizations = optimizer.optimize_memory_usage()
    
    # 6. æ•¸æ“šåŠ è¼‰å„ªåŒ–
    data_loading_optimizations = optimizer.setup_data_loading_optimization()
    
    # 7. å‰µå»ºè¨“ç·´é…ç½®
    training_config = optimizer.create_optimized_training_config()
    
    # 8. æ€§èƒ½åŸºæº–æ¸¬è©¦
    training_performance = optimizer.benchmark_training_performance(training_config)
    
    # 9. ç”Ÿæˆå„ªåŒ–å ±å‘Š
    optimization_report = optimizer.generate_optimization_report()
    
    # åˆä½µæ‰€æœ‰åŸºæº–æ¸¬è©¦çµæœ
    optimization_report["system_benchmarks"] = {
        "cpu_performance": cpu_benchmark,
        "memory_performance": memory_benchmark,
        "training_performance": training_performance
    }
    
    optimization_report["training_configuration"] = training_config
    optimization_report["memory_optimizations"] = memory_optimizations
    optimization_report["data_loading_optimizations"] = data_loading_optimizations
    
    # ä¿å­˜å ±å‘Š
    report_file = Path("mac_mps_optimization_report.json")
    
    def convert_types(obj):
        if isinstance(obj, (np.integer, int)):
            return int(obj)
        elif isinstance(obj, (np.floating, float)):
            return float(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_types(item) for item in obj]
        return obj
    
    report_serializable = convert_types(optimization_report)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_serializable, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š === å„ªåŒ–ç¸½çµ ===")
    print(f"   MPSå¯ç”¨: {'âœ…' if profiler.system_info.get('mps_available') else 'âŒ'}")
    print(f"   å„ªåŒ–é …ç›®: {len(optimizer.optimization_applied)} é …")
    print(f"   CPUè©•åˆ†: {cpu_benchmark['overall_score']:.1f}")
    print(f"   è¨“ç·´é€Ÿåº¦: {training_performance['samples_per_second']:.1f} æ¨£æœ¬/ç§’")
    print(f"   å»ºè­°æ‰¹æ¬¡å¤§å°: {training_config['batch_size']}")
    
    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")
    
    print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
    for rec in optimization_report["recommendations"]:
        print(f"   â€¢ {rec}")
    
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
    for step in optimization_report["next_steps"]:
        print(f"   â€¢ {step}")

if __name__ == "__main__":
    main()