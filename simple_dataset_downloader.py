#!/usr/bin/env python3
"""
OCR0712 ç²¾ç°¡ç‰ˆæ•¸æ“šé›†ä¸‹è¼‰å™¨ - ç„¡å¤–éƒ¨ä¾è³´
æä¾›å¯¦éš›å¯ç”¨çš„ä¸‹è¼‰é€£çµå’Œè™•ç†æ–¹æ³•
"""

import os
import urllib.request
import zipfile
import json
from pathlib import Path
import time

class SimpleDatasetDownloader:
    """ç²¾ç°¡ç‰ˆæ•¸æ“šé›†ä¸‹è¼‰å™¨"""
    
    def __init__(self, base_dir="./real_datasets"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        # å¯¦éš›å¯ç”¨çš„ä¸‹è¼‰URL
        self.dataset_urls = {
            "github_traditional_chinese": {
                "url": "https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset/archive/refs/heads/main.zip",
                "filename": "Traditional-Chinese-Handwriting.zip",
                "description": "ç¹é«”ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›† - 5,162å€‹æ¨£æœ¬",
                "size": "ç´„45MB"
            },
            
            "github_thu_hcr": {
                "url": "https://github.com/thu-ml/thu-hcr/archive/refs/heads/main.zip",
                "filename": "THU-HCR-Dataset.zip", 
                "description": "æ¸…è¯å¤§å­¸æ‰‹å¯«è­˜åˆ¥æ•¸æ“šé›†",
                "size": "ç´„30MB"
            },
            
            "sample_chinese_chars": {
                "url": "https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt",
                "filename": "chinese_characters_sample.txt",
                "description": "ä¸­æ–‡å­—ç¬¦åœ–å½¢æ•¸æ“šæ¨£æœ¬",
                "size": "ç´„5MB"
            }
        }
    
    def download_file(self, url, filename):
        """ç°¡å–®æ–‡ä»¶ä¸‹è¼‰"""
        filepath = self.base_dir / filename
        
        if filepath.exists():
            print(f"âœ“ æ–‡ä»¶å·²å­˜åœ¨: {filename}")
            return str(filepath)
        
        try:
            print(f"ğŸ”„ é–‹å§‹ä¸‹è¼‰: {filename}")
            print(f"ğŸ“¡ URL: {url}")
            
            # æ·»åŠ User-Agenté¿å…è¢«é˜»æ“‹
            req = urllib.request.Request(url)
            req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)')
            
            with urllib.request.urlopen(req, timeout=30) as response:
                total_size = int(response.headers.get('Content-Length', 0))
                
                with open(filepath, 'wb') as f:
                    downloaded = 0
                    while True:
                        chunk = response.read(8192)
                        if not chunk:
                            break
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            print(f"\rğŸ“¥ ä¸‹è¼‰é€²åº¦: {percent:.1f}% ({downloaded:,}/{total_size:,} bytes)", end='')
                    
                    print()  # æ›è¡Œ
            
            print(f"âœ… ä¸‹è¼‰å®Œæˆ: {filename}")
            return str(filepath)
            
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰å¤±æ•— {filename}: {e}")
            return None
    
    def extract_zip(self, zip_path):
        """è§£å£“ZIPæ–‡ä»¶"""
        print(f"ğŸ“¦ è§£å£“ç¸®: {Path(zip_path).name}")
        
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                extract_path = self.base_dir / Path(zip_path).stem
                zip_ref.extractall(extract_path)
            
            print(f"âœ… è§£å£“å®Œæˆåˆ°: {extract_path}")
            return extract_path
            
        except Exception as e:
            print(f"âŒ è§£å£“å¤±æ•—: {e}")
            return None
    
    def download_github_datasets(self):
        """ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›†"""
        print("\nğŸ  === ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›† ===")
        
        success_count = 0
        total_count = 0
        
        for dataset_key in ["github_traditional_chinese", "github_thu_hcr"]:
            total_count += 1
            dataset_info = self.dataset_urls[dataset_key]
            
            print(f"\nğŸ“‹ æ•¸æ“šé›†: {dataset_info['description']}")
            print(f"ğŸ“Š å¤§å°: {dataset_info['size']}")
            
            # ä¸‹è¼‰æ–‡ä»¶
            filepath = self.download_file(
                dataset_info["url"], 
                dataset_info["filename"]
            )
            
            # è§£å£“ç¸®
            if filepath and filepath.endswith('.zip'):
                extract_path = self.extract_zip(filepath)
                if extract_path:
                    self.analyze_dataset(extract_path)
                    success_count += 1
        
        print(f"\nâœ… GitHubæ•¸æ“šé›†ä¸‹è¼‰: {success_count}/{total_count} æˆåŠŸ")
        return success_count > 0
    
    def download_sample_data(self):
        """ä¸‹è¼‰æ¨£æœ¬æ•¸æ“š"""
        print("\nğŸ“ === ä¸‹è¼‰æ¨£æœ¬æ•¸æ“š ===")
        
        dataset_info = self.dataset_urls["sample_chinese_chars"]
        print(f"ğŸ“‹ {dataset_info['description']}")
        
        filepath = self.download_file(
            dataset_info["url"],
            dataset_info["filename"]
        )
        
        if filepath:
            self.process_sample_data(filepath)
            return True
        
        return False
    
    def analyze_dataset(self, dataset_path):
        """åˆ†ææ•¸æ“šé›†å…§å®¹"""
        print(f"ğŸ” åˆ†ææ•¸æ“šé›†: {dataset_path.name}")
        
        # çµ±è¨ˆæ–‡ä»¶é¡å‹
        file_counts = {}
        total_size = 0
        
        for file_path in dataset_path.rglob('*'):
            if file_path.is_file():
                suffix = file_path.suffix.lower()
                file_counts[suffix] = file_counts.get(suffix, 0) + 1
                total_size += file_path.stat().st_size
        
        print(f"ğŸ“Š æ–‡ä»¶çµ±è¨ˆ:")
        for suffix, count in sorted(file_counts.items()):
            print(f"  {suffix or 'ç„¡å¾Œç¶´'}: {count} å€‹æ–‡ä»¶")
        
        print(f"ğŸ’¾ ç¸½å¤§å°: {total_size / (1024*1024):.1f} MB")
        
        # æŸ¥æ‰¾é‡è¦æ–‡ä»¶
        readme_files = list(dataset_path.rglob('README*'))
        data_files = list(dataset_path.rglob('*.json')) + list(dataset_path.rglob('*.csv'))
        image_files = list(dataset_path.rglob('*.png')) + list(dataset_path.rglob('*.jpg'))
        
        if readme_files:
            print(f"ğŸ“– èªªæ˜æ–‡ä»¶: {[f.name for f in readme_files]}")
        
        if data_files:
            print(f"ğŸ“„ æ•¸æ“šæ–‡ä»¶: {len(data_files)} å€‹")
            
        if image_files:
            print(f"ğŸ–¼ï¸  åœ–åƒæ–‡ä»¶: {len(image_files)} å€‹")
            
            # åˆ†æå¹¾å€‹åœ–åƒæ¨£æœ¬
            sample_count = min(3, len(image_files))
            print(f"ğŸ” æª¢æŸ¥ {sample_count} å€‹åœ–åƒæ¨£æœ¬:")
            
            for i, img_file in enumerate(image_files[:sample_count]):
                try:
                    file_size = img_file.stat().st_size
                    print(f"  âœ… {img_file.name}: {file_size:,} bytes")
                except Exception as e:
                    print(f"  âŒ {img_file.name}: {e}")
    
    def process_sample_data(self, text_file):
        """è™•ç†æ¨£æœ¬æ–‡æœ¬æ•¸æ“š"""
        print(f"ğŸ”„ è™•ç†æ¨£æœ¬æ•¸æ“š: {Path(text_file).name}")
        
        try:
            processed_dir = self.base_dir / "processed_samples"
            processed_dir.mkdir(exist_ok=True)
            
            # è®€å–æ–‡æœ¬æ–‡ä»¶
            with open(text_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            print(f"ğŸ“„ è®€å–äº† {len(lines)} è¡Œæ•¸æ“š")
            
            # è™•ç†å‰100è¡Œä½œç‚ºæ¨£æœ¬
            samples = []
            for i, line in enumerate(lines[:100]):
                line = line.strip()
                if line:
                    sample = {
                        'id': i,
                        'content': line,
                        'source': 'chinese_characters_sample',
                        'processed_time': time.time()
                    }
                    samples.append(sample)
            
            # ä¿å­˜è™•ç†å¾Œçš„æ¨£æœ¬
            output_file = processed_dir / "processed_samples.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(samples, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… è™•ç†å®Œæˆ: {len(samples)} å€‹æ¨£æœ¬ä¿å­˜åˆ° {output_file}")
            
        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•—: {e}")
    
    def create_download_summary(self):
        """å‰µå»ºä¸‹è¼‰æ‘˜è¦"""
        print("\nğŸ“‹ === å‰µå»ºä¸‹è¼‰æ‘˜è¦ ===")
        
        summary = {
            'download_time': time.time(),
            'base_directory': str(self.base_dir.absolute()),
            'available_datasets': [],
            'total_files': 0,
            'total_size_mb': 0
        }
        
        # æƒææ‰€æœ‰ä¸‹è¼‰çš„å…§å®¹
        for item in self.base_dir.iterdir():
            if item.is_file():
                file_size = item.stat().st_size
                summary['total_files'] += 1
                summary['total_size_mb'] += file_size / (1024 * 1024)
                
                summary['available_datasets'].append({
                    'name': item.name,
                    'type': 'file',
                    'size_mb': file_size / (1024 * 1024)
                })
                
            elif item.is_dir():
                # çµ±è¨ˆç›®éŒ„å…§å®¹
                dir_files = list(item.rglob('*'))
                dir_size = sum(f.stat().st_size for f in dir_files if f.is_file())
                
                summary['available_datasets'].append({
                    'name': item.name,
                    'type': 'directory',
                    'files': len([f for f in dir_files if f.is_file()]),
                    'size_mb': dir_size / (1024 * 1024)
                })
                
                summary['total_files'] += len([f for f in dir_files if f.is_file()])
                summary['total_size_mb'] += dir_size / (1024 * 1024)
        
        # ä¿å­˜æ‘˜è¦
        summary_file = self.base_dir / "download_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ä¸‹è¼‰æ‘˜è¦:")
        print(f"  ğŸ“ æ•¸æ“šé›†æ•¸é‡: {len(summary['available_datasets'])}")
        print(f"  ğŸ“„ ç¸½æ–‡ä»¶æ•¸: {summary['total_files']}")
        print(f"  ğŸ’¾ ç¸½å¤§å°: {summary['total_size_mb']:.1f} MB")
        print(f"  ğŸ“‹ æ‘˜è¦æ–‡ä»¶: {summary_file}")
        
        return summary

def show_dataset_info():
    """é¡¯ç¤ºæ•¸æ“šé›†ä¿¡æ¯"""
    print("ğŸ¯ === å¯ç”¨çš„ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›† ===")
    print()
    
    datasets_info = [
        {
            "name": "Traditional Chinese Handwriting Dataset",
            "source": "AI-FREE-Team (GitHub)",
            "samples": "5,162 ç¹é«”ä¸­æ–‡æ‰‹å¯«åœ–åƒ",
            "format": "PNG + JSONæ¨™è¨»",
            "license": "MIT",
            "url": "https://github.com/AI-FREE-Team/Traditional-Chinese-Handwriting-Dataset"
        },
        {
            "name": "THU-HCR Dataset", 
            "source": "æ¸…è¯å¤§å­¸ (GitHub)",
            "samples": "å­¸è¡“ç´šæ‰‹å¯«è­˜åˆ¥æ•¸æ“šé›†",
            "format": "å¤šç¨®æ ¼å¼",
            "license": "å­¸è¡“ä½¿ç”¨",
            "url": "https://github.com/thu-ml/thu-hcr"
        },
        {
            "name": "CASIA-HWDB",
            "source": "ä¸­ç§‘é™¢è‡ªå‹•åŒ–æ‰€",
            "samples": "3.9M æ‰‹å¯«æ¼¢å­— (éœ€è¨»å†Š)",
            "format": "GNTæ ¼å¼",
            "license": "å­¸è¡“ä½¿ç”¨",
            "url": "http://www.nlpr.ia.ac.cn/databases/handwriting/"
        }
    ]
    
    for i, dataset in enumerate(datasets_info, 1):
        print(f"{i}. {dataset['name']}")
        print(f"   ä¾†æº: {dataset['source']}")
        print(f"   æ¨£æœ¬: {dataset['samples']}")
        print(f"   æ ¼å¼: {dataset['format']}")
        print(f"   æˆæ¬Š: {dataset['license']}")
        print(f"   é€£çµ: {dataset['url']}")
        print()

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ === OCR0712 ç²¾ç°¡ç‰ˆæ•¸æ“šé›†ä¸‹è¼‰å™¨ ===")
    print()
    
    show_dataset_info()
    
    downloader = SimpleDatasetDownloader()
    
    print("è«‹é¸æ“‡ä¸‹è¼‰é¸é …:")
    print("1. ä¸‹è¼‰GitHubé–‹æºæ•¸æ“šé›† (æ¨è–¦)")
    print("2. ä¸‹è¼‰æ¨£æœ¬æ•¸æ“š")
    print("3. ä¸‹è¼‰å…¨éƒ¨å¯ç”¨æ•¸æ“š")
    print("4. åƒ…é¡¯ç¤ºæ•¸æ“šé›†ä¿¡æ¯")
    print("5. å‰µå»ºä¸‹è¼‰æ‘˜è¦")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1-5): ").strip()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ä¸‹è¼‰å·²å–æ¶ˆ")
        return
    
    success = False
    
    if choice == "1":
        success = downloader.download_github_datasets()
    elif choice == "2":
        success = downloader.download_sample_data()
    elif choice == "3":
        print("ğŸ”„ ä¸‹è¼‰å…¨éƒ¨æ•¸æ“š...")
        github_success = downloader.download_github_datasets()
        sample_success = downloader.download_sample_data()
        success = github_success or sample_success
    elif choice == "4":
        print("â„¹ï¸  æ•¸æ“šé›†ä¿¡æ¯å·²é¡¯ç¤ºåœ¨ä¸Šæ–¹")
        return
    elif choice == "5":
        downloader.create_download_summary()
        return
    else:
        print("âŒ ç„¡æ•ˆé¸æ“‡")
        return
    
    if success:
        print("\nğŸ“‹ å‰µå»ºä¸‹è¼‰æ‘˜è¦...")
        summary = downloader.create_download_summary()
        
        print("\nğŸ‰ === ä¸‹è¼‰å®Œæˆ ===")
        print(f"ğŸ“ æ•¸æ“šé›†ä½ç½®: {downloader.base_dir.absolute()}")
        print("\nğŸ’¡ å¾ŒçºŒæ­¥é©Ÿ:")
        print("   1. æª¢æŸ¥ä¸‹è¼‰çš„æ•¸æ“šé›†ç›®éŒ„")
        print("   2. æŸ¥çœ‹ download_summary.json äº†è§£è©³æƒ…")
        print("   3. é‹è¡Œ local_training_system.py é–‹å§‹è¨“ç·´")
        print("   4. ä½¿ç”¨ software_rl_gym.py é€²è¡ŒRLå„ªåŒ–")
        
        print(f"\nğŸ“ å¿«é€Ÿé–‹å§‹å‘½ä»¤:")
        print(f"   cd {downloader.base_dir}")
        print(f"   ls -la")
        print(f"   python3 ../local_training_system.py")
        
    else:
        print("\nâŒ ä¸‹è¼‰æœªå®Œæˆ")
        print("ğŸ’¡ å»ºè­°:")
        print("   1. æª¢æŸ¥ç¶²çµ¡é€£æ¥")
        print("   2. ç¨å¾Œé‡è©¦")
        print("   3. å˜—è©¦æ‰‹å‹•ä¸‹è¼‰æ•¸æ“šé›†")

if __name__ == "__main__":
    main()