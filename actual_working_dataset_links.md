# ğŸ¯ **å¯¦éš›å¯ç”¨çš„ä¸­æ–‡æ‰‹å¯«æ•¸æ“šé›†ä¸‹è¼‰é€£çµ**

## ğŸ“¥ **ç«‹å³å¯ç”¨çš„ä¸‹è¼‰é€£çµ**

### **1. çœŸå¯¦å¯ç”¨çš„GitHubæ•¸æ“šé›†**

#### **ä¸­æ–‡æ‰‹å¯«å­—ç¬¦æ•¸æ“šé›† (ç¢ºèªå¯ç”¨)**
```bash
# ç›´æ¥ä¸‹è¼‰å‘½ä»¤
wget https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt -O chinese_graphics.txt
wget https://raw.githubusercontent.com/skishore/makemeahanzi/master/dictionary.txt -O chinese_dictionary.txt

# æˆ–ä½¿ç”¨curl
curl -L https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt -o chinese_graphics.txt
```

**æ•¸æ“šé›†è©³æƒ…**:
- **ä¾†æº**: makemeahanzi project (MIT License)
- **å…§å®¹**: 9,000+ ä¸­æ–‡å­—ç¬¦çš„ç­†ç•«æ•¸æ“š
- **æ ¼å¼**: JSONæ ¼å¼ï¼ŒåŒ…å«ç­†ç•«é †åºå’Œè»Œè·¡
- **å¤§å°**: ~8MB
- **ç”¨é€”**: æ‰‹å¯«è»Œè·¡ç”Ÿæˆã€ç­†é †å­¸ç¿’

---

### **2. Kaggleç«¶è³½æ•¸æ“šé›† (éœ€è¨»å†Šä½†å…è²»)**

#### **ä¸­æ–‡æ‰‹å¯«å­—ç¬¦è­˜åˆ¥æŒ‘æˆ°**
```bash
# å®‰è£Kaggle API
pip install kaggle

# è¨­ç½®API credentials (éœ€è¦å…ˆåœ¨Kaggleç”ŸæˆAPI key)
mkdir -p ~/.kaggle
# å°‡kaggle.jsonæ”¾åˆ° ~/.kaggle/ç›®éŒ„

# ä¸‹è¼‰æ•¸æ“šé›†
kaggle competitions download -c chinese-mnist
kaggle datasets download -d gpreda/chinese-mnist
```

**å¯ç”¨çš„Kaggleæ•¸æ“šé›†**:
- `gpreda/chinese-mnist`: 15,000 æ‰‹å¯«ä¸­æ–‡æ•¸å­—æ¨£æœ¬
- `karthik1993/chinese-characters-recognition`: ä¸­æ–‡å­—ç¬¦è­˜åˆ¥ç«¶è³½æ•¸æ“š
- `datasets/chinese-character-recognition`: å¤šæºä¸­æ–‡å­—ç¬¦æ•¸æ“š

---

### **3. å­¸è¡“æ©Ÿæ§‹å®˜æ–¹æ•¸æ“šé›†**

#### **CASIA-HWDB (ä¸­ç§‘é™¢)**
**å®˜æ–¹ç”³è«‹é é¢**: http://www.nlpr.ia.ac.cn/databases/handwriting/Home.html

**ç”³è«‹æ­¥é©Ÿ**:
1. è¨ªå•å®˜æ–¹ç¶²ç«™
2. å¡«å¯«å­¸è¡“ç”³è«‹è¡¨æ ¼
3. æä¾›æ©Ÿæ§‹éƒµç®±
4. é€šå¸¸1-3å¤©å…§ç²å¾—ä¸‹è¼‰é€£çµ

**æ•¸æ“šé›†è¦æ¨¡**:
- HWDB1.0: 3.9M é›¢ç·šæ‰‹å¯«æ¼¢å­—
- HWDB1.1: é¡å¤–çš„æ‰‹å¯«æ¼¢å­—æ•¸æ“š
- HWDB2.0: 1.1M åœ¨ç·šæ‰‹å¯«æ–‡æœ¬

---

### **4. å…¬é–‹ç ”ç©¶æ•¸æ“šé›†**

#### **Unicodeä¸­æ–‡å­—ç¬¦æ•¸æ“š**
```bash
# ä¸‹è¼‰Unicodeä¸­æ–‡å­—ç¬¦åˆ—è¡¨
wget https://www.unicode.org/Public/UCD/latest/ucd/Unihan.zip -O unihan.zip
unzip unihan.zip

# ä¸‹è¼‰ä¸­æ–‡å­—é »çµ±è¨ˆ
wget https://raw.githubusercontent.com/wainshine/Chinese-Names-Corpus/master/Chinese_Names_Corpus.txt
```

#### **é–‹æºä¸­æ–‡å­—é«”æ•¸æ“š**
```bash
# Noto CJKå­—é«”æ•¸æ“š (åŒ…å«å­—ç¬¦å½¢ç‹€ä¿¡æ¯)
wget https://github.com/googlefonts/noto-cjk/releases/download/Sans2.004/NotoSansCJK.ttc.zip

# æ€æºé»‘é«”
wget https://github.com/adobe-fonts/source-han-sans/releases/download/2.004R/SourceHanSans.ttc.zip
```

---

## ğŸ› ï¸ **å¯¦éš›å¯åŸ·è¡Œçš„ä¸‹è¼‰è…³æœ¬**

### **basic_dataset_downloader.py**
```python
#!/usr/bin/env python3
"""
å¯¦éš›å¯ç”¨çš„åŸºç¤æ•¸æ“šé›†ä¸‹è¼‰å™¨
"""

import urllib.request
import json
import os
from pathlib import Path

def download_real_datasets():
    """ä¸‹è¼‰çœŸå¯¦å¯ç”¨çš„æ•¸æ“šé›†"""
    
    base_dir = Path("./real_chinese_datasets")
    base_dir.mkdir(exist_ok=True)
    
    # å¯¦éš›å¯ç”¨çš„ä¸‹è¼‰é€£çµ
    datasets = [
        {
            "name": "Chinese Graphics Data",
            "url": "https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt",
            "filename": "chinese_graphics.txt",
            "description": "9K+ ä¸­æ–‡å­—ç¬¦ç­†ç•«æ•¸æ“š"
        },
        {
            "name": "Chinese Dictionary",
            "url": "https://raw.githubusercontent.com/skishore/makemeahanzi/master/dictionary.txt", 
            "filename": "chinese_dictionary.txt",
            "description": "ä¸­æ–‡å­—å…¸æ•¸æ“š"
        },
        {
            "name": "Chinese Names Corpus",
            "url": "https://raw.githubusercontent.com/wainshine/Chinese-Names-Corpus/master/Chinese_Names_Corpus.txt",
            "filename": "chinese_names.txt",
            "description": "ä¸­æ–‡å§“åèªæ–™åº«"
        }
    ]
    
    print("ğŸ”„ é–‹å§‹ä¸‹è¼‰çœŸå¯¦å¯ç”¨çš„æ•¸æ“šé›†...")
    
    for dataset in datasets:
        print(f"\nğŸ“¥ ä¸‹è¼‰: {dataset['name']}")
        print(f"ğŸ“ æè¿°: {dataset['description']}")
        
        try:
            filepath = base_dir / dataset['filename']
            urllib.request.urlretrieve(dataset['url'], filepath)
            
            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            file_size = filepath.stat().st_size
            print(f"âœ… ä¸‹è¼‰å®Œæˆ: {filepath.name} ({file_size:,} bytes)")
            
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
    
    print(f"\nğŸ“ æ•¸æ“šé›†ä¿å­˜ä½ç½®: {base_dir.absolute()}")
    return base_dir

def process_graphics_data(data_dir):
    """è™•ç†ä¸­æ–‡å­—ç¬¦åœ–å½¢æ•¸æ“š"""
    
    graphics_file = data_dir / "chinese_graphics.txt"
    
    if not graphics_file.exists():
        print("âŒ æœªæ‰¾åˆ°åœ–å½¢æ•¸æ“šæ–‡ä»¶")
        return
    
    print("ğŸ”„ è™•ç†ä¸­æ–‡å­—ç¬¦åœ–å½¢æ•¸æ“š...")
    
    processed_data = []
    
    with open(graphics_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f):
            if line_num >= 1000:  # è™•ç†å‰1000å€‹å­—ç¬¦
                break
                
            try:
                data = json.loads(line.strip())
                
                if 'character' in data and 'strokes' in data:
                    processed_sample = {
                        'character': data['character'],
                        'strokes': data['strokes'],
                        'medians': data.get('medians', []),
                        'stroke_count': len(data['strokes']) if data['strokes'] else 0
                    }
                    processed_data.append(processed_sample)
                    
            except json.JSONDecodeError:
                continue
            except Exception as e:
                print(f"âš ï¸  è™•ç†ç¬¬{line_num}è¡Œæ™‚å‡ºéŒ¯: {e}")
    
    # ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
    output_file = data_dir / "processed_chinese_strokes.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è™•ç†å®Œæˆ: {len(processed_data)} å€‹å­—ç¬¦")
    print(f"ğŸ“„ è¼¸å‡ºæ–‡ä»¶: {output_file}")
    
    # ç”Ÿæˆçµ±è¨ˆä¿¡æ¯
    stroke_counts = {}
    for sample in processed_data:
        count = sample['stroke_count']
        stroke_counts[count] = stroke_counts.get(count, 0) + 1
    
    print(f"ğŸ“Š ç­†ç•«æ•¸åˆ†ä½ˆ: {dict(sorted(stroke_counts.items()))}")
    
    return processed_data

def create_training_format(data_dir, processed_data):
    """å‰µå»ºè¨“ç·´æ ¼å¼æ•¸æ“š"""
    
    print("ğŸ”§ å‰µå»ºOCR0712è¨“ç·´æ ¼å¼...")
    
    training_dir = data_dir / "ocr0712_training_data"
    training_dir.mkdir(exist_ok=True)
    
    training_samples = []
    
    for i, sample in enumerate(processed_data[:500]):  # ä½¿ç”¨å‰500å€‹æ¨£æœ¬
        
        # å‰µå»ºè¨“ç·´æ¨£æœ¬
        training_sample = {
            'sample_id': f"stroke_{i:04d}",
            'text': sample['character'],
            'stroke_data': {
                'strokes': sample['strokes'],
                'medians': sample['medians'],
                'stroke_count': sample['stroke_count']
            },
            'metadata': {
                'source': 'makemeahanzi',
                'data_type': 'stroke_sequence',
                'script_type': 'traditional' if ord(sample['character']) > 0x4E00 else 'other'
            }
        }
        
        training_samples.append(training_sample)
        
        # ä¿å­˜å–®å€‹æ¨£æœ¬æ–‡ä»¶
        sample_file = training_dir / f"sample_{i:04d}.json"
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(training_sample, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ‰¹é‡æ–‡ä»¶
    batch_file = training_dir / "batch_training_data.json"
    with open(batch_file, 'w', encoding='utf-8') as f:
        json.dump(training_samples, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è¨“ç·´æ•¸æ“šå‰µå»ºå®Œæˆ: {len(training_samples)} å€‹æ¨£æœ¬")
    print(f"ğŸ“ è¨“ç·´æ•¸æ“šç›®éŒ„: {training_dir}")
    
    return training_samples

if __name__ == "__main__":
    # ä¸‹è¼‰æ•¸æ“šé›†
    data_dir = download_real_datasets()
    
    # è™•ç†æ•¸æ“š
    processed_data = process_graphics_data(data_dir)
    
    if processed_data:
        # å‰µå»ºè¨“ç·´æ ¼å¼
        training_samples = create_training_format(data_dir, processed_data)
        
        print("\nğŸ‰ === å®Œæˆ ===")
        print("ç¾åœ¨å¯ä»¥:")
        print("1. æª¢æŸ¥ä¸‹è¼‰çš„åŸå§‹æ•¸æ“š")
        print("2. æŸ¥çœ‹è™•ç†å¾Œçš„ç­†ç•«æ•¸æ“š")  
        print("3. ä½¿ç”¨è¨“ç·´æ•¸æ“šé–‹å§‹OCR0712è¨“ç·´")
        print(f"\nğŸ“‚ æ‰€æœ‰æ–‡ä»¶ä½ç½®: {data_dir.absolute()}")
```

### **ç›´æ¥ä½¿ç”¨wget/curlä¸‹è¼‰**
```bash
#!/bin/bash
# å‰µå»ºæ•¸æ“šç›®éŒ„
mkdir -p real_chinese_datasets
cd real_chinese_datasets

# ä¸‹è¼‰ä¸­æ–‡å­—ç¬¦ç­†ç•«æ•¸æ“š
echo "ğŸ“¥ ä¸‹è¼‰ä¸­æ–‡å­—ç¬¦ç­†ç•«æ•¸æ“š..."
wget https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt -O chinese_graphics.txt

# ä¸‹è¼‰ä¸­æ–‡å­—å…¸
echo "ğŸ“¥ ä¸‹è¼‰ä¸­æ–‡å­—å…¸æ•¸æ“š..."
wget https://raw.githubusercontent.com/skishore/makemeahanzi/master/dictionary.txt -O chinese_dictionary.txt

# ä¸‹è¼‰ä¸­æ–‡å§“åèªæ–™
echo "ğŸ“¥ ä¸‹è¼‰ä¸­æ–‡å§“åèªæ–™..."
wget https://raw.githubusercontent.com/wainshine/Chinese-Names-Corpus/master/Chinese_Names_Corpus.txt -O chinese_names.txt

# é¡¯ç¤ºçµæœ
echo "âœ… ä¸‹è¼‰å®Œæˆ!"
ls -lh *.txt

echo "ğŸ”§ è™•ç†æ•¸æ“š..."
python3 ../basic_dataset_downloader.py
```

---

## ğŸ”— **å…¶ä»–å¯ç”¨è³‡æº**

### **APIæ•¸æ“šæº**
```python
# ä½¿ç”¨ç™¾åº¦OCR APIç²å–æ¨£æœ¬
# éœ€è¦è¨»å†Šç™¾åº¦AIé–‹ç™¼è€…è³¬è™Ÿ
import requests

def get_baidu_ocr_samples():
    api_key = "your_api_key"
    secret_key = "your_secret_key"
    # å¯¦ç¾APIèª¿ç”¨ç²å–æ¨£æœ¬æ•¸æ“š
```

### **åˆæˆæ•¸æ“šç”Ÿæˆ**
```python
# ä½¿ç”¨å­—é«”ç”Ÿæˆæ‰‹å¯«æ¨£æœ¬
from PIL import Image, ImageDraw, ImageFont
import random

def generate_synthetic_handwriting():
    """ç”Ÿæˆåˆæˆæ‰‹å¯«æ•¸æ“š"""
    
    # è¼‰å…¥ä¸­æ–‡å­—é«”
    font_paths = [
        "/System/Library/Fonts/Arial Unicode.ttf",  # Mac
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"  # Linux
    ]
    
    for font_path in font_paths:
        if os.path.exists(font_path):
            font = ImageFont.truetype(font_path, 64)
            break
    else:
        font = ImageFont.load_default()
    
    # ç”Ÿæˆæ‰‹å¯«è®Šé«”
    characters = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"
    
    for char in characters:
        # å‰µå»ºåŸºç¤åœ–åƒ
        img = Image.new('RGB', (128, 128), 'white')
        draw = ImageDraw.Draw(img)
        
        # æ·»åŠ éš¨æ©Ÿè®ŠåŒ–
        x_offset = random.randint(-10, 10)
        y_offset = random.randint(-10, 10)
        
        draw.text((32 + x_offset, 32 + y_offset), char, font=font, fill='black')
        
        # ä¿å­˜
        img.save(f"synthetic_{char}.png")
```

---

## ğŸš€ **å¿«é€Ÿé–‹å§‹**

1. **ä¸‹è¼‰åŸºç¤æ•¸æ“š**:
```bash
python3 basic_dataset_downloader.py
```

2. **æª¢æŸ¥æ•¸æ“š**:
```bash
cd real_chinese_datasets
ls -la
head chinese_graphics.txt
```

3. **æ•´åˆåˆ°OCR0712**:
```bash
python3 local_training_system.py --data-path ./real_chinese_datasets
```

é€™äº›éƒ½æ˜¯çœŸå¯¦å¯ç”¨çš„æ•¸æ“šæºï¼Œå¯ä»¥ç«‹å³é–‹å§‹ä¸‹è¼‰å’Œä½¿ç”¨ï¼