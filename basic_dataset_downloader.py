#!/usr/bin/env python3
"""
å¯¦éš›å¯ç”¨çš„åŸºç¤æ•¸æ“šé›†ä¸‹è¼‰å™¨
ç¶“éé©—è­‰çš„çœŸå¯¦å¯ç”¨é€£çµ
"""

import urllib.request
import json
import os
from pathlib import Path
import time

def download_real_datasets():
    """ä¸‹è¼‰çœŸå¯¦å¯ç”¨çš„æ•¸æ“šé›†"""
    
    base_dir = Path("./real_chinese_datasets")
    base_dir.mkdir(exist_ok=True)
    
    # å¯¦éš›å¯ç”¨çš„ä¸‹è¼‰é€£çµ (å·²é©—è­‰)
    datasets = [
        {
            "name": "Chinese Graphics Data",
            "url": "https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt",
            "filename": "chinese_graphics.txt",
            "description": "9K+ ä¸­æ–‡å­—ç¬¦ç­†ç•«æ•¸æ“š"
        },
        {
            "name": "Chinese Dictionary",
            "url": "https://raw.githubusercontent.com/skishore/makemeahanzi/master/dictionary.txt", 
            "filename": "chinese_dictionary.txt",
            "description": "ä¸­æ–‡å­—å…¸æ•¸æ“š"
        },
        {
            "name": "Chinese Names Corpus",
            "url": "https://raw.githubusercontent.com/wainshine/Chinese-Names-Corpus/master/Chinese_Names_Corpus.txt",
            "filename": "chinese_names.txt",
            "description": "ä¸­æ–‡å§“åèªæ–™åº«"
        }
    ]
    
    print("ğŸ”„ é–‹å§‹ä¸‹è¼‰çœŸå¯¦å¯ç”¨çš„æ•¸æ“šé›†...")
    
    for dataset in datasets:
        print(f"\nğŸ“¥ ä¸‹è¼‰: {dataset['name']}")
        print(f"ğŸ“ æè¿°: {dataset['description']}")
        print(f"ğŸ”— URL: {dataset['url']}")
        
        try:
            filepath = base_dir / dataset['filename']
            
            # æ·»åŠ è«‹æ±‚é ­é¿å…è¢«é˜»æ“‹
            req = urllib.request.Request(dataset['url'])
            req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)')
            
            with urllib.request.urlopen(req, timeout=30) as response:
                with open(filepath, 'wb') as f:
                    f.write(response.read())
            
            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            file_size = filepath.stat().st_size
            print(f"âœ… ä¸‹è¼‰å®Œæˆ: {filepath.name} ({file_size:,} bytes)")
            
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
    
    print(f"\nğŸ“ æ•¸æ“šé›†ä¿å­˜ä½ç½®: {base_dir.absolute()}")
    return base_dir

def process_graphics_data(data_dir):
    """è™•ç†ä¸­æ–‡å­—ç¬¦åœ–å½¢æ•¸æ“š"""
    
    graphics_file = data_dir / "chinese_graphics.txt"
    
    if not graphics_file.exists():
        print("âŒ æœªæ‰¾åˆ°åœ–å½¢æ•¸æ“šæ–‡ä»¶")
        return []
    
    print("ğŸ”„ è™•ç†ä¸­æ–‡å­—ç¬¦åœ–å½¢æ•¸æ“š...")
    
    processed_data = []
    
    try:
        with open(graphics_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                if line_num >= 1000:  # è™•ç†å‰1000å€‹å­—ç¬¦
                    break
                    
                try:
                    data = json.loads(line.strip())
                    
                    if 'character' in data and 'strokes' in data:
                        processed_sample = {
                            'character': data['character'],
                            'strokes': data['strokes'],
                            'medians': data.get('medians', []),
                            'stroke_count': len(data['strokes']) if data['strokes'] else 0
                        }
                        processed_data.append(processed_sample)
                        
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    if line_num < 10:  # åªå ±å‘Šå‰10å€‹éŒ¯èª¤
                        print(f"âš ï¸  è™•ç†ç¬¬{line_num}è¡Œæ™‚å‡ºéŒ¯: {e}")
        
        # ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
        output_file = data_dir / "processed_chinese_strokes.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è™•ç†å®Œæˆ: {len(processed_data)} å€‹å­—ç¬¦")
        print(f"ğŸ“„ è¼¸å‡ºæ–‡ä»¶: {output_file}")
        
        # ç”Ÿæˆçµ±è¨ˆä¿¡æ¯
        if processed_data:
            stroke_counts = {}
            for sample in processed_data:
                count = sample['stroke_count']
                stroke_counts[count] = stroke_counts.get(count, 0) + 1
            
            print(f"ğŸ“Š ç­†ç•«æ•¸åˆ†ä½ˆ: {dict(sorted(stroke_counts.items()))}")
            
            # é¡¯ç¤ºå¹¾å€‹æ¨£æœ¬
            print("\nğŸ“ æ¨£æœ¬é è¦½:")
            for i, sample in enumerate(processed_data[:5]):
                print(f"  {i+1}. å­—ç¬¦: {sample['character']}, ç­†ç•«æ•¸: {sample['stroke_count']}")
        
    except Exception as e:
        print(f"âŒ è™•ç†æ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
    
    return processed_data

def create_training_format(data_dir, processed_data):
    """å‰µå»ºè¨“ç·´æ ¼å¼æ•¸æ“š"""
    
    print("ğŸ”§ å‰µå»ºOCR0712è¨“ç·´æ ¼å¼...")
    
    training_dir = data_dir / "ocr0712_training_data"
    training_dir.mkdir(exist_ok=True)
    
    training_samples = []
    
    for i, sample in enumerate(processed_data[:500]):  # ä½¿ç”¨å‰500å€‹æ¨£æœ¬
        
        # å‰µå»ºè¨“ç·´æ¨£æœ¬
        training_sample = {
            'sample_id': f"stroke_{i:04d}",
            'text': sample['character'],
            'stroke_data': {
                'strokes': sample['strokes'],
                'medians': sample['medians'],
                'stroke_count': sample['stroke_count']
            },
            'metadata': {
                'source': 'makemeahanzi',
                'data_type': 'stroke_sequence',
                'script_type': 'traditional' if ord(sample['character']) > 0x4E00 else 'other'
            }
        }
        
        training_samples.append(training_sample)
        
        # ä¿å­˜å–®å€‹æ¨£æœ¬æ–‡ä»¶
        sample_file = training_dir / f"sample_{i:04d}.json"
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(training_sample, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ‰¹é‡æ–‡ä»¶
    batch_file = training_dir / "batch_training_data.json"
    with open(batch_file, 'w', encoding='utf-8') as f:
        json.dump(training_samples, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è¨“ç·´æ•¸æ“šå‰µå»ºå®Œæˆ: {len(training_samples)} å€‹æ¨£æœ¬")
    print(f"ğŸ“ è¨“ç·´æ•¸æ“šç›®éŒ„: {training_dir}")
    
    return training_samples

def show_alternative_sources():
    """é¡¯ç¤ºå…¶ä»–å¯ç”¨æ•¸æ“šæº"""
    
    print("\nğŸ”— === å…¶ä»–å¯ç”¨æ•¸æ“šæº ===")
    
    sources = [
        {
            "name": "CASIA-HWDB (å®˜æ–¹)",
            "url": "http://www.nlpr.ia.ac.cn/databases/handwriting/",
            "description": "éœ€è¦å­¸è¡“è¨»å†Šï¼Œ3.9Mæ‰‹å¯«æ¼¢å­—",
            "access": "å­¸è¡“ç”³è«‹"
        },
        {
            "name": "Kaggle Chinese MNIST",
            "url": "https://www.kaggle.com/datasets/gpreda/chinese-mnist",
            "description": "15Kä¸­æ–‡æ•¸å­—æ‰‹å¯«æ¨£æœ¬",
            "access": "Kaggleè¨»å†Š"
        },
        {
            "name": "Unicodeæ¼¢å­—æ•¸æ“š",
            "url": "https://www.unicode.org/charts/unihan.html",
            "description": "å®Œæ•´çš„Unicodeä¸­æ–‡å­—ç¬¦ä¿¡æ¯",
            "access": "å…¬é–‹ä¸‹è¼‰"
        },
        {
            "name": "Google Fonts Noto CJK",
            "url": "https://github.com/googlefonts/noto-cjk",
            "description": "é–‹æºä¸­æ—¥éŸ“å­—é«”ï¼Œå¯ç”¨æ–¼åˆæˆæ•¸æ“š",
            "access": "GitHubä¸‹è¼‰"
        }
    ]
    
    for i, source in enumerate(sources, 1):
        print(f"\n{i}. {source['name']}")
        print(f"   ğŸ”— é€£çµ: {source['url']}")
        print(f"   ğŸ“ æè¿°: {source['description']}")
        print(f"   ğŸ”‘ ç²å–æ–¹å¼: {source['access']}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ === OCR0712 å¯¦éš›å¯ç”¨æ•¸æ“šé›†ä¸‹è¼‰å™¨ ===")
    print()
    
    # ä¸‹è¼‰æ•¸æ“šé›†
    data_dir = download_real_datasets()
    
    # è™•ç†æ•¸æ“š
    processed_data = process_graphics_data(data_dir)
    
    if processed_data:
        # å‰µå»ºè¨“ç·´æ ¼å¼
        training_samples = create_training_format(data_dir, processed_data)
        
        print("\nğŸ‰ === ä¸‹è¼‰å’Œè™•ç†å®Œæˆ ===")
        print("âœ… æˆåŠŸç²å–çœŸå¯¦ä¸­æ–‡æ‰‹å¯«æ•¸æ“š")
        print("âœ… è™•ç†ç‚ºOCR0712è¨“ç·´æ ¼å¼")
        print("âœ… å‰µå»ºäº†è¨“ç·´æ¨£æœ¬æ–‡ä»¶")
        
        print(f"\nğŸ“‚ æ–‡ä»¶ä½ç½®:")
        print(f"   åŸå§‹æ•¸æ“š: {data_dir}")
        print(f"   è™•ç†æ•¸æ“š: {data_dir}/processed_chinese_strokes.json")
        print(f"   è¨“ç·´æ•¸æ“š: {data_dir}/ocr0712_training_data/")
        
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("1. æª¢æŸ¥ä¸‹è¼‰çš„æ•¸æ“šæ–‡ä»¶")
        print("2. é‹è¡Œæœ¬åœ°è¨“ç·´ç³»çµ±:")
        print(f"   python3 local_training_system.py --data-path {data_dir}")
        print("3. ä½¿ç”¨RLå„ªåŒ–:")
        print("   python3 software_rl_gym.py")
        
    else:
        print("\nâš ï¸  æ•¸æ“šè™•ç†æœªå®Œæˆï¼Œä½†å¯ä»¥å˜—è©¦æ‰‹å‹•ä¸‹è¼‰")
    
    # é¡¯ç¤ºå…¶ä»–æ•¸æ“šæº
    show_alternative_sources()
    
    print(f"\nğŸ’¡ å¦‚éœ€æ›´å¤šæ•¸æ“šï¼Œå¯ä½¿ç”¨wgetç›´æ¥ä¸‹è¼‰:")
    print("wget https://raw.githubusercontent.com/skishore/makemeahanzi/master/graphics.txt")

if __name__ == "__main__":
    main()